1.duration：时长

2.nextInt：https://blog.csdn.net/qq_43513083/article/details/93862121

```java
//通话时间
int year = 2017;
//月份(0~11)
int month = r.nextInt(12);
//天,范围(1~31)
int day = r.nextInt(29) + 1;
int hour = r.nextInt(24);
int min = r.nextInt(60);
int sec = r.nextInt(60);
```

3.dfs.applyPattern("yyyy/MM/dd HH:mm:ss");

4.yarn

```shell
查询yarn的命令
yarn rmadmin

单独启动机器上的resourcrmanager
yarn-daemon.sh start resourcemanager
```

5。Hive在hadoop中的目录：

user/hive/warehouse

6.hive和mysql同时存在的情况下，如果mysql重装了的话，那么需要初始化hive

```shell
hdfs dfs -rmr /user/hive

在mysql里创建hive库

cd /hive/bin
./schematool -dbType mysql -initSchema

验证：
进入mysql
use hive;
show tables;//查看里面是否有表
select * from dbs;//是否为空
在hive里建一个数据库，然后回到mysql的dbs表里，如果有hdfs://mycuster/user/hive/warehouse说明就是成功的
```

#### 7.在hive中创建的表可以存储在hbase中，不过hbase要先有表和它映射,这样在hive中就可以看到Hbase中存储的数据了

```
hive>create external table ext_calllogs_in_hbase(id string, caller string,callTime string,callee string,callDuration string) 
STORED BY
'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f1:caller,f1:callTime,f1:callee,f1:callDuration") 
TBLPROPERTIES ("hbase.table.name" = "ns1:calllogs");
```

8.datax:python ./bin/datax.py ./job/mysql2hdfs.json

9.awk：awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分在进行各种处理。

```shell
jps | awk  '{print $1}': jps进程，打印第一列
```

![1635688526082](C:\Users\SHENHAI\AppData\Local\Temp\1635688526082.png)

```shell
kill -9 `jps | grep kafka | awk '{print $1}'`
```

![1635688763842](C:\Users\SHENHAI\AppData\Local\Temp\1635688763842.png)

grep:过滤

|：管道

动态打印ip地址：

![1635689067520](C:\Users\SHENHAI\AppData\Local\Temp\1635689067520.png)

10.Echarts数据可视化

11.11