1.阿里云每天停机不收费后第二天开机公有ip会发生变化，私有ip不会发生变化，所以要去hosts文件改一下。

2.分发文件

```shell
scp /etc/hosts hadoop102:/etc/
```

分发目录

```
scp -r /opt/module/jdk1.8.0_211/ hadoop102:/opt/module/
```

3.hive报错了，就查tmp底下的hive.log文件，如果看不出问题就要到yarn上查看

4.守护进程启动kafka

```
/opt/module/kafka_2.11-2.4.0/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.0/config/server.properties
```

5.sqoop是分布式的，消耗yarn资源；datax是单机的，不走yarn，比较占网络带宽，安装时要限制网速不然会沾满，网速越快传的越快。

6.kudu是数据库，适合做OLAP分析；mysql不能存储海量数据，需要做索引

7.apache、CDH、HDP：CDH和HDP合并为CDP，CDH免费的最高版本6.3.2

8.建模工具powerdesigner

9.IDEA运行spark报错

```
Exception in thread "main" org.apache.spark.SparkException: A master URL must be set in your configuration
```

解决方法：https://blog.csdn.net/qq_21383435/article/details/77200167

10.在本地跑添加setMaster("local[*]")，放到集群中去就要注释掉，就近原则，他的优先级别会比命令行的高

11.把代码打包放到集群当中去跑yarn模式，要修改pom.xml，因为集群中都有spark环境，所以打包的时候没有必要打一个包含spark的包，加上这个标签   <scope>provided</scope>   的意思就是编译和打包的时候都不会把这个依赖列进去

12.打包时win中和IDEA中的scala的版本要一样

https://blog.csdn.net/weixin_34302798/article/details/85975934

13.alibaba的fastjson处理json文件的

14.linux中写.sh文件格式一定要注意，放大弄

15.spark

controller是入口类 --submit

service处理一个逻辑

dao层写sql

16.Datax：https://github.com/alibaba/DataX

17.从hive中导数据到mysql进行可视化之前要在mysql里建和hive中对应的空表

18.HIve的数据实际上就是存在HDFS上

19.datax数据类型可以都写成string类型



```shell
python /opt/module/datax/bin/datax.py paper_maxdetail.json -p "-Ddt=20190722 -Ddn=webA"
```

-p:表示要传参，用“ ”表示

20.所有的命令比如sparksubmit和datax，如果需要批量执行的时候直接写一个脚本一次性执行即可

把执行命令的路径写正确极客，如19所示

21.不管是Hive还是mysql，都需要在里面先建表，导入和传递的只是数据

22.各大核心组件可以在github上直接搜，什么都有，还有例子和源码

23.内存优化

RDD：kryo+series

DF、DS：

```
.serializer.KryoSerializer
```

24.优化，在数仓实现文档中

25.sparkui：4040

26.yarn：8088

27.小标join大表数据倾斜

28.spark Streaming 是核心Spark API的扩展，可实现实时数据的可扩展，高吞吐量，容错处理。数据可以从许多来源（<u>如Kafka,Flume,Kinesis,或TCP套接字</u>）中获取，在内部，他的工作原理如下，Spark Streaming接收实时输入数据流并将数据分成批处理，然后由Spark引擎处理以批处理生成最终结果流。

29.binglog：实时监控数据库的数据库变化 日志信息

Oracle OGG（Oracle自带的服务）收费的

mysql canal没有断点续传的功能，要搭建HA，maxwell有断点续传的功能（自带）

MongDb 2.0 oplog           4.0 change stream JAVA代码（官方自带的组件）

30.kafka，首先要创建topic，在生产和消费，后期回顾一下

31.sparkstreaming实时计算之前写过例子，就是学校教的那个大作业，看一下

32.

如果自动提交

提交偏移量这一步骤是立马执行，不会等业务处理完毕再提交的

那么就会造成数据丢失情况



精准一次消费

手动维护偏移

等业务数据处理完了之后，再去提交偏移量

33.day01中有从头开始配hadoop集群