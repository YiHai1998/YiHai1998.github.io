# æœåŠ¡è§„åˆ’

åœ¨éƒ¨ç½²ç»„ä»¶ä¹‹å‰ï¼Œè¯·ç¡®è®¤å½“å‰ç£ç›˜ç©ºé—´æ˜¯å¦è¶³å¤Ÿï¼

| æœåŠ¡åç§°     | ç‰ˆæœ¬  | åŠŸèƒ½                                        | å­æœåŠ¡                | æœåŠ¡å™¨      | æœåŠ¡å™¨      | æœåŠ¡å™¨      |
| ------------ | ----- | ------------------------------------------- | --------------------- | ----------- | ----------- | ----------- |
|              |       |                                             |                       | tsingdata01 | tsingdata02 | tsingdata03 |
| Hadoopâ€”â€”HDFS | 3.1.3 | ä½¿ç”¨Hiveçš„å‰æ                              | NameNode              | âˆš           |             |             |
|              |       |                                             | DataNode              | âˆš           | âˆš           | âˆš           |
|              |       |                                             | SecondaryNameNode     |             |             | âˆš           |
| Hadoopâ€”â€”Yarn | 3.1.3 |                                             | NodeManager           | âˆš           | âˆš           | âˆš           |
|              |       |                                             | Resourcemanager       |             | âˆš           |             |
|              |       |                                             |                       |             |             |             |
| Zookeeper    | 3.5.7 | Kafkaçš„å‰æï¼Œå­˜å‚¨äº†Brokerä¿¡æ¯å’Œæ¶ˆè´¹è€…ä¿¡æ¯ã€‚ | QuorumPeerMain        | âˆš           | âˆš           | âˆš           |
| Kafka        | 2.4.1 | è¯»å–å…ƒæ•°æ®ç®¡ç†                              | Kafka                 | âˆš           | âˆš           | âˆš           |
| HBase        | 2.0.5 | å­˜å‚¨å…ƒæ•°æ®                                  | HMaster               | âˆš           |             |             |
|              |       |                                             | HRegionServer         | âˆš           | âˆš           | âˆš           |
| Solr         | 5.2.1 | æ£€ç´¢ã€æŸ¥è¯¢å…ƒæ•°æ®                            | Jar                   | âˆš           | âˆš           | âˆš           |
| Hive         | 3.1.2 |                                             | Hive                  | âˆš           |             |             |
| MySQL        | 5.7   | å­˜å‚¨Hiveçš„å…ƒæ•°æ®                            | MySQL                 | âˆš           |             |             |
| Azkaban      | 3.8.4 | è°ƒåº¦                                        | AzkabanWebServer      | âˆš           |             |             |
|              |       |                                             | AzkabanExecutorServer | âˆš           |             |             |
| Atlas        | 2.0   |                                             | Atlas                 | âˆš           |             |             |
| æœåŠ¡æ•°æ€»è®¡   |       |                                             |                       | 13          | 7           | 7           |

# Java8

```
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
vim /etc/profile
# Java
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JRE_HOME=$JAVA_HOME/jre
source /etc/profile
[root@hadoop01 jdk1.8.0_212]# java -version
java version "1.8.0_212"
Java(TM) SE Runtime Environment (build 1.8.0_212-b10)
Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)
```

# DataX

https://github.com/alibaba/DataX

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621494982555-d04ab997-5d04-4117-842a-ef57e91ecc1a.png)

è§£å‹å®‰è£…

ç¤ºä¾‹

```
python2 bin/datax.py job/job.json
```

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621496393295-329a3d30-69ec-45cb-86f5-bda302f0e385.png)

æ¨¡æ¿

```
python2 bin/datax.py -r streamreader -w streamwriter\
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "streamreader", 
                    "parameter": {
                        "column": [], 
                        "sliceRecordCount": ""
                    }
                }, 
                "writer": {
                    "name": "streamwriter", 
                    "parameter": {
                        "encoding": "", 
                        "print": true
                    }
                }
            }
        ], 
        "setting": {
            "speed": {
                "channel": ""
            }
        }
    }
}
```

åœ¨jobä¸‹åˆ›å»ºstream2stream.json

```
{
  "job": {
    "content": [
      {
        "reader": {
          "name": "streamreader",
          "parameter": {
            "sliceRecordCount": 10,
            "column": [
              {
                "type": "long",
                "value": "10"
              },
              {
                "type": "string",
                "value": "helloï¼Œä½ å¥½ï¼Œæ¸…æ•°ç ”ç©¶é™¢"
              }
            ]
          }
        },
        "writer": {
          "name": "streamwriter",
          "parameter": {
            "encoding": "UTF-8",
            "print": true
          }
        }
      }
    ],
    "setting": {
      "speed": {
        "channel": 5
       }
    }
  }
}
python2 bin/datax.py job/stream2stream.json
```



## MySQLæ•°æ®æº

åœ¨mysqlåˆ›å»ºè¡¨ï¼Œå¹¶æ·»åŠ 

```
create database test;
use test;
create table stu(id int, name varchar(255));

insert into stu(id, name)
values
(1, 'zhangsan'),
(2, 'lisi'),
(3, 'wangwu');
vim mysql2hdfs.json
{
    "job": {
        "setting": {
            "speed": {
                 "channel": 3
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "root",
                        "column": [
                            "id",
                            "name"
                        ],
                        "connection": [
                            {
                                "table": [
                                    "stu"
                                ],
                                "jdbcUrl": [
     "jdbc:mysql://127.0.0.1:3306/test"
                                ]
                            }
                        ]
                    }
                },
 "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://tsingdata01:8020",
                        "fileType": "text",
                        "path": "/",
                        "fileName": "student.txt",
                        "column": [
                            {
                                "name": "id",
                                "type": "int"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\t",
                    }
                }
            }
        ]
    }
}
```



```
python2 bin/datax.py job/mysql2hdfs.json
```

http://192.168.28.116:9870/explorer.html#/

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621504220044-a61aa850-0bde-433f-87ba-9051aaa0b391.png)

## DataX-webå¯è§†åŒ–

https://github.com/WeiYe-Jing/datax-web

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621495031834-d0e8af70-31aa-4b79-8001-a5ead7d76af5.png)

å®‰è£…æ•™ç¨‹ï¼šhttps://github.com/WeiYe-Jing/datax-web/blob/master/doc/datax-web/datax-web-deploy.md



è®¿é—®ï¼šhttp://192.168.28.116:9527/index.html

# Hadoop

```
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
vim /etc/profile
# Hadoop
export HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source /etc/profile
[root@hadoop01 jdk1.8.0_212]# hadoop version
Hadoop 3.1.3
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar
```

## ä¼ªåˆ†å¸ƒå¼å¯åŠ¨

### é…ç½®

```
cd /opt/module/hadoop-3.1.3/etc/hadoop
vim core-site.xml
		<property>
        <name>hadoop.tmp.dir</name>
        <value>file:/opt/module/hadoop-3.1.3/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
vim hdfs-site.xml
		<property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/opt/module/hadoop-3.1.3/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/opt/module/hadoop-3.1.3/tmp/dfs/data</value>
		</property>
```

ä¼ªåˆ†å¸ƒå¼è™½ç„¶åªéœ€è¦é…ç½® fs.defaultFS å’Œ dfs.replication å°±å¯ä»¥è¿è¡Œï¼ˆå®˜æ–¹æ•™ç¨‹å¦‚æ­¤ï¼‰ï¼Œä¸è¿‡è‹¥æ²¡æœ‰é…ç½® hadoop.tmp.dir å‚æ•°ï¼Œåˆ™é»˜è®¤ä½¿ç”¨çš„ä¸´æ—¶ç›®å½•ä¸º /tmp/hadoo-hadoopï¼Œè€Œè¿™ä¸ªç›®å½•åœ¨é‡å¯æ—¶æœ‰å¯èƒ½è¢«ç³»ç»Ÿæ¸…ç†æ‰ï¼Œå¯¼è‡´å¿…é¡»é‡æ–°æ‰§è¡Œ format æ‰è¡Œã€‚æ‰€ä»¥æˆ‘ä»¬è¿›è¡Œäº†è®¾ç½®ï¼ŒåŒæ—¶ä¹ŸæŒ‡å®š **dfs.namenode.name.dir** å’Œ **dfs.datanode.data.dir**ï¼Œå¦åˆ™åœ¨æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­å¯èƒ½ä¼šå‡ºé”™ã€‚

#### é…ç½®masterå’Œslaveè¿æ¥

è¿è¡Œå¦‚ä¸‹æŒ‡ä»¤ï¼Œå¹¶ä¸”ä¸€ç›´å›è½¦

```
ssh-keygen -t rsa 
The key fingerprint is:
SHA256:W5zT7FkSO/9V5mqjivzM35I+P9z/gtLS+uvZSJ+OXho root@hadoop01
The key's randomart image is:
+---[RSA 2048]----+
|                 |
|                 |
|            .    |
|         . + o   |
|        S = * . o|
|         o o * o.|
|        .  oE+o.o|
|       . +o.B@*+o|
|        o.*@&OO+*|
+----[SHA256]-----+
```

è¿è¡Œå¦‚ä¸‹ä»£ç 

```
cd /root/.ssh
cat id_rsa.pub >> authorized_keys
```

#### æ‰§è¡Œ NameNode çš„æ ¼å¼åŒ–

é…ç½®å®Œæˆåï¼Œæ‰§è¡Œ NameNode çš„æ ¼å¼åŒ–

```
cd /opt/module/hadoop
bin/hdfs namenode -format
```

æˆåŠŸçš„è¯ï¼Œä¼šçœ‹åˆ° â€œsuccessfully formattedâ€ çš„æç¤º



ä¿®æ”¹start-dfs.shï¼Œstop-dfs.shæ–‡ä»¶ï¼Œæ·»åŠ 

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs 
HDFS_NAMENODE_USER=root 
HDFS_SECONDARYNAMENODE_USER=root
```



### å¯åŠ¨

```
start-dfs.sh
```

æŸ¥çœ‹jps

```
(base) [root@glong hadoop]# jps
26804 SecondaryNameNode
26344 NameNode
26506 DataNode
26971 Jps
```

è®¿é—®http://ip:9870/

### å…³é—­

```
stop-dfs.sh
```

## é›†ç¾¤å¯åŠ¨

### é…ç½®

hadoopå…±éœ€é…ç½®6ä¸ªåœ°æ–¹

```
cd /opt/module/hadoop-3.1.3/etc/hadoop
```



#### hadoop-env.sh

```
vim hadoop-env.sh
```



æ·»åŠ å¦‚ä¸‹é…ç½®

```
export JAVA_HOME=/opt/module/jdk1.8.0_212

export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export HDFS_ZKFC_USER=root
export HDFS_JOURNALNODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```



#### core-site.xml

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

 <property>
        <name>fs.defaultFS</name>
        <value>hdfs://tsingdata01:8020</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>tsingdata</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>

</configuration>
```



#### hdfs-site.xml

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>/opt/module/hadoop-3.1.3/data/dfs/nn</value>
</property>
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>tsingdata03:9868</value>
</property>
   <!-- datanodeç»“ç‚¹è¢«æŒ‡å®šè¦å­˜å‚¨æ•°æ®çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ -->
<property>
   <name>dfs.datanode.data.dir</name>
   <value>/opt/module/hadoop-3.1.3/data/dfs/dn</value>
</property>

<!-- æµ‹è¯•ç¯å¢ƒæŒ‡å®šHDFSå‰¯æœ¬çš„æ•°é‡1 -->
<property>
    <name>dfs.replication</name>
    <value>3</value>
</property>

</configuration>
```



#### yarn-site.xml



```
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>


<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>tsingdata02</value>
</property>
<property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>

<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„ç‰©ç†å†…å­˜é‡ï¼Œå¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼ï¼Œåˆ™ç›´æ¥å°†å…¶æ€æ‰ï¼Œé»˜è®¤æ˜¯true -->
<property>
   <name>yarn.nodemanager.pmem-check-enabled</name>
   <value>false</value>
</property>

<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„è™šæ‹Ÿå†…å­˜é‡ï¼Œå¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼ï¼Œåˆ™ç›´æ¥å°†å…¶æ€æ‰ï¼Œé»˜è®¤æ˜¯true -->
<property>
   <name>yarn.nodemanager.vmem-check-enabled</name>
   <value>false</value>
</property>

</configuration>
```



#### mapred-site.xml



```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

<!-- å†å²æœåŠ¡å™¨ç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>tsingdata01:10020</value>
</property>

<!-- å†å²æœåŠ¡å™¨webç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>tsingdata01:19888</value>
</property>

</configuration>
```



#### workers

åœ¨`etc/hadoop/workers`ç§æ·»åŠ 

```
tsingdata01
tsingdata02
tsingdata03
```



#### æœ€åï¼šxsyncè„šæœ¬åˆ†å‘

è¯¥è„šæœ¬åœ¨ï¼šxsync

åˆ†å‘é…ç½®æ–‡ä»¶åˆ°å…¶ä»–æœºå™¨

```
xsync /opt/module/hadoop/etc/hadoop
```



### æ ¼å¼åŒ–

å¦‚æœé›†ç¾¤æ˜¯ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼Œéœ€è¦æ ¼å¼åŒ– ï¼ˆåªè¦æ ¼å¼åŒ–1æ¬¡å°±è¡Œï¼Œä¸€å®šä¸è¦å†æ ¼å¼åŒ–ï¼ï¼ï¼ï¼‰NameNodeï¼ˆæ³¨æ„æ ¼å¼åŒ–ä¹‹å‰ï¼Œä¸€å®šè¦å…ˆåœæ­¢ä¸Šæ¬¡å¯åŠ¨çš„æ‰€æœ‰ namenode å’Œ datanode è¿›ç¨‹ï¼Œç„¶åå†åˆ é™¤ data å’Œ log æ•°æ®ï¼‰



```
 bin/hdfs namenode -format
```



### å¯åŠ¨

```
start-dfs.sh
```



å¦‚æœæƒ³è¦ä½¿ç”¨è„šæœ¬ï¼Œè¯·å‚è€ƒ**è„šæœ¬**è¿™ä¸€ç« 



# mysql



```
# æŸ¥çœ‹æœ‰æ²¡æœ‰åŸå§‹mysql
rpm -qa|grep mysql

# æœ‰å°±å¸è½½
yum remove mysql-community-server-5.6.36-2.el7.x86_64
rpm -qa | grep -i mysql

# å®‰è£…
rpm -ivh 01_mysql-community-common-5.7.16-1.el7.x86_64.rpm
rpm -ivh 02_mysql-community-libs-5.7.16-1.el7.x86_64.rpm
rpm -ivh 03_mysql-community-libs-compat-5.7.16-1.el7.x86_64.rpm 
rpm -ivh 04_mysql-community-client-5.7.16-1.el7.x86_64.rpm
rpm -ivh 05_mysql-community-server-5.7.16-1.el7.x86_64.rpm

......

# å¯åŠ¨mysql
systemctl start mysqld

# å¯†ç 
grep 'temporary password' /var/log/mysqld.log
å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯·å‚è€ƒï¼š https://blog.csdn.net/pao___pao/article/details/80118904

# å…ˆç™»å½•ï¼Œç„¶åè®¾ç½®å¯†ç 
mysql -uroot -pVyrrhBd/E3ij

# mysqlè¦æ±‚è®¾ç½®å¤æ‚å¯†ç ï¼Œæ‰€ä»¥è¦ä¿®æ”¹å¯†ç ç­–ç•¥
set password=password("Qs23=zs32");
set global validate_password_length=4;
set global validate_password_policy=0;
set password=password("root");

# è¿›å…¥mysqlåº“
use mysql
# æŸ¥è¯¢userè¡¨
select user,host from user;
# ä¿®æ”¹user.è¡¨ï¼ŒæŠŠHostè¡¨å†…å®¹ä¿®æ”¹ä¸º%
update user set host="%" where user="root";
# åˆ·æ–°
flush privileges;
# é€€å‡º
quit;
```

# Hive

etc/profile

```
# Hive
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
```

source /etc/profile

è§£å‹å®‰è£…



å¦‚æœåç»­åˆ é™¤hiveå¹¶é‡æ–°å®‰è£…ï¼Œè¯·è®°å¾—å…ˆåˆ é™¤mysqlä¸­çš„metastoreæ•°æ®åº“ã€‚



```
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module
mv apache-hive-3.1.2-bin hive
```



ä¸‹è½½`mysql-connector-java-5.1.48.tar.gz`



```
tar -zxvf mysql-connector-java-5.1.48.tar.gz
```



ç„¶åå°†`mysql-connector-java-5.1.48`ä¸­çš„`mysql-connector-java-5.1.48.jar`æ”¾å…¥åˆ°hive/libä¸­



```
cp mysql-connector-java-5.1.48.jar /opt/module/hive/lib/

chmod 777 /opt/module/hive/lib/mysql-connector-java-5.1.48.jar
```



```
cd hive/conf
mv hive-env.sh.template hive-env.sh

# è§£å†³æ—¥å¿—JaråŒ…å†²çªï¼Œè¿›å…¥opt/module/hive/libç›®å½•â†
mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
```



```
vim hive-env.sh
# æ·»åŠ 
HADOOP_HOME=/opt/module/hadoop-3.1.3
export HIVE_CONF_DIR=/opt/module/hadoop-3.1.3/conf
```



```
vim hive-site.xml
```



ç‰¹åˆ«è¦æ³¨æ„ä¸»æœºå`tsingdata01`ã€`mysqlç”¨æˆ·å`ã€`mysqlå¯†ç `ï¼Œæ ¹æ®ä½ è‡ªå·±å®é™…æƒ…å†µæ¥ï¼Œä¸€å®šè¦åœ¨mysqlåˆ›å»º`metastoreæ•°æ®åº“`ï¼



```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    
       <!--hive çš„å…ƒæ•°æ®æœåŠ¡, ä¾›spark SQL ä½¿ç”¨-->
    <property>
        ã€€ã€€ã€€ã€€<name>hive.metastore.uris</name>
        ã€€ã€€ã€€ã€€<value>thrift://tsingdata01:9083</value>
        ã€€ã€€ã€€ã€€<description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
        ã€€ã€€</property>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://tsingdata01:3306/metastore?useSSL=false</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>root</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>

    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>tsingdata01</value>
    </property>

    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
    
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
</configuration>
```



```
chmod 777 hive-site.xml
```



åœ¨ HDFS ä¸Šåˆ›å»ºHiveæ‰€éœ€è·¯å¾„ï¼Œ`/tmp/hive`å’Œ`/user/hive/warehouse` ä¸¤ä¸ªç›®å½•å¹¶ä¿®æ”¹ä»–ä»¬çš„åŒç»„æƒé™å¯å†™ã€‚

```
hadoop fs -mkdir /tmp/hive
hadoop fs -mkdir -p /user/hive/warehouse
hadoop fs -chmod 777 /tmp/hive
hadoop fs -chmod 777 /user/hive/warehouse
```



http://192.168.157.128:9870/explorer.html#/	æŸ¥çœ‹

mysqlåˆ›å»º`metastore`ï¼Œè¿™ä¸ªmetastoreå¯¹åº”äº†hive-site.xmlä¸­çš„é…ç½®

```
create database metastore;

grant all privileges on *.* to 'root'@'localhost' identified by 'root';

#å…è®¸ç”¨æˆ·rootä»localhostçš„ä¸»æœºè¿æ¥åˆ°mysqlæœåŠ¡å™¨ï¼Œå¹¶ä½¿ç”¨rootä½œä¸ºå¯†ç 
grant all privileges on *.* to 'root'@'%' identified by 'root';

#rootä½¿ç”¨rootå¯†ç ä»ä»»ä½•ä¸»æœºè¿æ¥åˆ°mysqlæœåŠ¡å™¨
flush privileges;
```



æŸ¥çœ‹æ•°æ®åº“`metastore`çš„æƒé™

```
SELECT host,user,Grant_priv,Super_priv FROM mysql.user;
+-----------+-----------+------------+------------+
| host      | user      | Grant_priv | Super_priv |
+-----------+-----------+------------+------------+
| %         | root      | Y          | Y          |
| localhost | mysql.sys | N          | N          |
| localhost | root      | N          | Y          |
+-----------+-----------+------------+------------+
```

ä¸‹é¢å¯åŠ¨æœåŠ¡å™¨

åˆå§‹åŒ–å…ƒæ•°æ®

```
schematool -initSchema -dbType mysql -verbose
```



æˆåŠŸäº†ä¼šåœ¨metestoreæ•°æ®åº“ä¸­å»ºå¾ˆå¤šè¡¨



`æŠ¥é”™`ï¼š



Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V



`è§£å†³æ–¹æ³•`ï¼š



è¿™æ˜¯å› ä¸ºå½“æ—¶ç”¨çš„hive2.xï¼Œä¸hadoop3.xä¸å…¼å®¹ã€‚ä¸‹é¢æ–¹æ³•ä¸è¦ä½¿ç”¨ï¼Œç›´æ¥å‡çº§åˆ°hive3ï¼Œå› ä¸ºå³ä½¿è§£å†³äº†è¯¥é—®é¢˜ï¼Œè¿˜ä¼šæœ‰æ–°çš„é—®é¢˜å‡ºç°ã€‚



åˆ é™¤hiveä¸­ä½ç‰ˆæœ¬çš„`guava-14.0.1.jar`åŒ…ï¼Œå°†hadoop-3.1.3/share/hadoop/common/libä¸­çš„`guava-27.0-jre.jar`å¤åˆ¶åˆ°hiveçš„libç›®å½•ä¸‹å³å¯ã€‚



![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618882226162-1ea4c5ed-bb3d-4f1b-80b6-7eb599e0f939.png)

```
cd hadoop-3.1.3/share/hadoop/common/lib
cp guava-27.0-jre.jar /opt/module/hive/lib
cd /opt/module/hive/lib
rm -rf guava-14.0.1.jar
```



å¯åŠ¨

```
nohup hive --service metastore &
nohup hive --service hiveserver2&	# å¦‚æœç”¨datagripç­‰å·¥å…·éœ€è¦å¼€å¯

hive
```



show databases; `æŠ¥é”™`ï¼šjava.lang.RuntimeException: Error in configuring object



åŸå› æ˜¯Hiveçš„ä¾èµ–åº“ä¸­ç¼ºå°‘lzoçš„jaråŒ…ã€‚

è§£å†³åŠæ³•ï¼š



æŠŠlzoçš„jaråŒ…æ”¾åˆ°${HIVE_HOME}/libç›®å½•ä¸‹ã€‚



cp hadoop-lzo-0.4.20.jar ${HIVE_HOME}/lib



ä»‹ç»ä¸€ä¸‹å¸¸ç”¨çš„å‘½ä»¤



```
æ‰“å¼€é»˜è®¤æ•°æ®åº“ 
	hive> use default; 
æ˜¾ç¤º default æ•°æ®åº“ä¸­çš„è¡¨ 
	hive> show tables; 
åˆ›å»ºä¸€å¼ è¡¨ 
	hive> create table student(id int, name string); 
æ˜¾ç¤ºæ•°æ®åº“ä¸­æœ‰å‡ å¼ è¡¨
	hive> show tables; 
æŸ¥çœ‹è¡¨çš„ç»“æ„ 
	hive> desc student; 
å‘è¡¨ä¸­æ’å…¥æ•°æ® 
	hive> insert into student values(1000,"ss"); 
æŸ¥è¯¢è¡¨ä¸­æ•°æ® 
	hive> select * from student; 
é€€å‡º hive 
	hive> quit;
```

# Zookeeper

### ZKé›†ç¾¤éƒ¨ç½²

#### è§£å‹å®‰è£…

æ‹·è´Zookeeperå®‰è£…åŒ…åˆ°Linuxç³»ç»Ÿä¸‹

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618907180565-0daadc7f-533a-45f7-b156-09ad581e75dd.png)

æˆ–è€…ä¸‹è½½ï¼šwget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.5.7/apache-zookeeper-3.5.7-bin.tar.gz

è§£å‹åˆ°æŒ‡å®šç›®å½•

```
tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
```

åŒæ­¥/opt/module/zookeeper-3.5.7ç›®å½•å†…å®¹åˆ°**tsingdata02**ã€**tsingdata03**

```
cd /opt/module
mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
xsync zookeeper-3.5.7/
```

#### é…ç½®æœåŠ¡å™¨ç¼–å·

ï¼ˆ1ï¼‰åœ¨/opt/module/zookeeper-3.5.7/è¿™ä¸ªç›®å½•ä¸‹åˆ›å»ºzkData

```
mkdir -p zkData
```

ï¼ˆ2ï¼‰åœ¨/opt/module/zookeeper-3.5.7/zkDataç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªmyidçš„æ–‡ä»¶

```
touch myid
```



ï¼ˆ3ï¼‰ç¼–è¾‘myidæ–‡ä»¶

æ³¨æ„ï¼šæ·»åŠ myidæ–‡ä»¶ï¼Œæ³¨æ„ä¸€å®šè¦åœ¨linuxé‡Œé¢åˆ›å»ºï¼Œåœ¨notepad++ç­‰ç¼–è¾‘å™¨é‡Œé¢å¾ˆå¯èƒ½**ä¹±ç ã€‚**

```
vim myid
```

â€‹	åœ¨æ–‡ä»¶ä¸­æ·»åŠ ä¸serverå¯¹åº”çš„ç¼–å·ï¼š

```
1
```

ï¼ˆ4ï¼‰æ‹·è´é…ç½®å¥½çš„zookeeperåˆ°å…¶ä»–æœºå™¨ä¸Š

```
xsync myid
```

å¹¶åˆ†åˆ«åœ¨**tsingdata02**ã€**tsingdata03**ä¸Šä¿®æ”¹myidæ–‡ä»¶ä¸­å†…å®¹ä¸º**2**ã€**3**ã€‚



#### é…ç½®ä¿®æ”¹

```
cd /opt/module/zookeeper-3.5.7/bin
vim zkEnv.sh
```

\# æ·»åŠ 

JAVA_HOME="/opt/module/jdk1.8.0_212"

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618973649437-678d25a7-e3ba-412b-8202-527eaefe4d38.png)

xsync zkEnv.sh

å°†/opt/module/zookeeper-3.5.7/confè¿™ä¸ªè·¯å¾„ä¸‹çš„zoo_sample.cfgä¿®æ”¹ä¸ºzoo.cfgï¼›

```
mv zoo_sample.cfg zoo.cfg
```

æ‰“å¼€zoo.cfgæ–‡ä»¶ï¼Œä¿®æ”¹dataDirè·¯å¾„ï¼š

```
vim zoo.cfg
```

ä¿®æ”¹å¦‚ä¸‹å†…å®¹

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618910571869-0a7643c4-a434-4624-832a-1ca2b75963c1.png)

```
dataDir=/opt/module/zookeeper-3.5.7/zkData
dataLogDir=/opt/module/zookeeper-3.5.7/logs
```

å¹¶ä¸”åœ¨æœ«å°¾å¢åŠ å¦‚ä¸‹é…ç½®

```
#######################cluster##########################
server.1=tsingdata01:2888:3888
server.2=tsingdata02:2888:3888
server.3=tsingdata03:2888:3888
```

é…ç½®å‚æ•°è§£è¯»ï¼š

**server.A=B:C:D**

å…¶ä¸­ï¼š

**A**æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºè¿™ä¸ªæ˜¯ç¬¬å‡ å·æœåŠ¡å™¨ï¼›

é›†ç¾¤æ¨¡å¼ä¸‹é…ç½®ä¸€ä¸ªæ–‡ä»¶myidï¼Œè¿™ä¸ªæ–‡ä»¶åœ¨dataDirç›®å½•ä¸‹ï¼Œè¿™ä¸ªæ–‡ä»¶é‡Œé¢æœ‰ä¸€ä¸ªæ•°æ®å°±æ˜¯Açš„å€¼ï¼ŒZookeeperå¯åŠ¨æ—¶è¯»å–æ­¤æ–‡ä»¶ï¼Œæ‹¿åˆ°é‡Œé¢çš„æ•°æ®ä¸zoo.cfgé‡Œé¢çš„é…ç½®ä¿¡æ¯æ¯”è¾ƒä»è€Œåˆ¤æ–­åˆ°åº•æ˜¯å“ªä¸ªserverã€‚

**B**æ˜¯è¿™ä¸ªæœåŠ¡å™¨çš„ipåœ°å€ï¼›

**C**æ˜¯è¿™ä¸ªæœåŠ¡å™¨ä¸é›†ç¾¤ä¸­çš„LeaderæœåŠ¡å™¨äº¤æ¢ä¿¡æ¯çš„ç«¯å£ï¼›

**D**æ˜¯ä¸‡ä¸€é›†ç¾¤ä¸­çš„LeaderæœåŠ¡å™¨æŒ‚äº†ï¼Œéœ€è¦ä¸€ä¸ªç«¯å£æ¥é‡æ–°è¿›è¡Œé€‰ä¸¾ï¼Œé€‰å‡ºä¸€ä¸ªæ–°çš„Leaderï¼Œè€Œè¿™ä¸ªç«¯å£å°±æ˜¯ç”¨æ¥æ‰§è¡Œé€‰ä¸¾æ—¶æœåŠ¡å™¨ç›¸äº’é€šä¿¡çš„ç«¯å£ã€‚



åŒæ­¥zoo.cfgé…ç½®æ–‡ä»¶

```
xsync zoo.cfg
```

####  æ“ä½œZookeeper

ï¼ˆ1ï¼‰å¯åŠ¨Zookeeper

```
bin/zkServer.sh start
```

ï¼ˆ2ï¼‰æŸ¥çœ‹è¿›ç¨‹æ˜¯å¦å¯åŠ¨

jps

4020 Jps

4001 QuorumPeerMain

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618910690164-9073de78-8c4e-411f-ae6f-6da87bd71c5f.png)

ï¼ˆ3ï¼‰æŸ¥çœ‹çŠ¶æ€ï¼š

```
bin/zkServer.sh status
```

ZooKeeper JMX enabled by default

Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg

Client port found: 2181. Client address: localhost.

ï¼ˆ4ï¼‰å¯åŠ¨å®¢æˆ·ç«¯ï¼š

```
bin/zkCli.sh
```

ï¼ˆ5ï¼‰é€€å‡ºå®¢æˆ·ç«¯ï¼š

```
quit
```

ï¼ˆ6ï¼‰åœæ­¢Zookeeper

```
bin/zkServer.sh stop
```



# Kafka

### Kafkaç¯å¢ƒå‡†å¤‡

jaråŒ…ä¸‹è½½ï¼šhttp://kafka.apache.org/downloads.html

### Kafkaé›†ç¾¤éƒ¨ç½²

ä¸Šä¼ å®‰è£…åŒ…åˆ°åœ¨/opt/softwareå¤„

è§£å‹å®‰è£…åŒ…

```
tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/
```

ä¿®æ”¹è§£å‹åçš„æ–‡ä»¶åç§°

```
cd /opt/module
mv kafka_2.11-2.4.1/ kafka
```

åœ¨/opt/module/kafkaç›®å½•ä¸‹åˆ›å»ºlogsæ–‡ä»¶å¤¹

```
mkdir logs
```

#### é…ç½®ä¿®æ”¹

```
cd config/
vim server.properties
```

ä¿®æ”¹ä»¥ä¸‹å†…å®¹ï¼ˆé‡ç‚¹ä¿®æ”¹ä¸‹é¢ä¸¤ä¸ªå›¾çš„å†…å®¹ï¼‰ï¼š

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618910939603-c0a9561f-3961-4291-941f-85f2e66976fa.png)

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618911029670-bc80e172-f23a-4913-a1aa-a8c32912ab21.png)

```
#brokerçš„å…¨å±€å”¯ä¸€ç¼–å·ï¼Œä¸èƒ½é‡å¤
broker.id=0
#åˆ é™¤topicåŠŸèƒ½ä½¿èƒ½
delete.topic.enable=true
#å¤„ç†ç½‘ç»œè¯·æ±‚çš„çº¿ç¨‹æ•°é‡
num.network.threads=3
#ç”¨æ¥å¤„ç†ç£ç›˜IOçš„ç°æˆæ•°é‡
num.io.threads=8
#å‘é€å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°
socket.send.buffer.bytes=102400
#æ¥æ”¶å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°
socket.receive.buffer.bytes=102400
#è¯·æ±‚å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°
socket.request.max.bytes=104857600
#kafkaè¿è¡Œæ—¥å¿—å­˜æ”¾çš„è·¯å¾„ 
log.dirs=/opt/module/kafka/logs
#topicåœ¨å½“å‰brokerä¸Šçš„åˆ†åŒºä¸ªæ•°
num.partitions=1
#ç”¨æ¥æ¢å¤å’Œæ¸…ç†dataä¸‹æ•°æ®çš„çº¿ç¨‹æ•°é‡
num.recovery.threads.per.data.dir=1
#segmentæ–‡ä»¶ä¿ç•™çš„æœ€é•¿æ—¶é—´ï¼Œè¶…æ—¶å°†è¢«åˆ é™¤
log.retention.hours=168
#é…ç½®è¿æ¥Zookeeperé›†ç¾¤åœ°å€
zookeeper.connect=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```

é…ç½®ç¯å¢ƒå˜é‡

vim /etc/profile

```
#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```

source /etc/profile

åˆ†å‘å®‰è£…åŒ…

xsync kafka/

æ³¨æ„ï¼šåˆ†å‘ä¹‹åè®°å¾—é…ç½®å…¶ä»–æœºå™¨çš„ç¯å¢ƒå˜é‡

åˆ†åˆ«åœ¨tsingdata02å’Œtsingdata03ä¸Šä¿®æ”¹é…ç½®æ–‡ä»¶/opt/module/kafka/config/server.propertiesä¸­çš„broker.id=1ã€broker.id=2

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618911278811-e8a89b1b-eedf-46a6-9993-fa9d54c5cc8b.png)

æ³¨æ„ï¼šbroker.idä¸å¾—é‡å¤

#### å¯åŠ¨

å¯åŠ¨é›†ç¾¤ï¼ˆè„šæœ¬å¯åŠ¨è§æœ¬æ–‡æ¡£æœ€å**è„šæœ¬**è¿™ä¸€ç« èŠ‚ï¼‰

ä¾æ¬¡åœ¨tsingdata01ã€tsingdata02ã€tsingdata03èŠ‚ç‚¹ä¸Šå¯åŠ¨kafka

```
bin/kafka-server-start.sh config/server.properties &
```

ä¾æ¬¡åœ¨tsingdata01ã€tsingdata02ã€tsingdata03èŠ‚ç‚¹ä¸Šå…³é—­kafkaï¼ˆè„šæœ¬å…³é—­è§æœ¬æ–‡æ¡£çš„**è„šæœ¬**è¿™ä¸€ç« èŠ‚ï¼‰

```
bin/kafka-server-stop.sh stop
```

# HBase

### HBaseéƒ¨ç½²

ä¸Šä¼ /opt/software

è§£å‹HBaseåˆ°æŒ‡å®šç›®å½•ï¼š

```
tar -zxvf hbase-2.0.5-bin.tar.gz -C /opt/module
```

#### HBaseçš„é…ç½®æ–‡ä»¶

ä¿®æ”¹HBaseå¯¹åº”çš„é…ç½®æ–‡ä»¶ã€‚

1ï¼‰/opt/module/hbase-2.0.5/conf/hbase-env.shæ·»åŠ å†…å®¹ï¼š

```
export JAVA_HOME=/opt/module/jdk1.8.0_212
export HBASE_MANAGES_ZK=false
```

2ï¼‰hbase-site.xmlä¿®æ”¹å†…å®¹ï¼š



```
<configuration>
	<property>     
		<name>hbase.rootdir</name>     
		<value>hdfs://tsingdata01:8020/hbase</value>   
	</property>

     <!-- é…ç½®å¯ç”¨é›†ç¾¤åˆ†å¸ƒå¼è¿è¡Œ -->
	<property>   
		<name>hbase.cluster.distributed</name>
		<value>true</value>
	</property>

   <!-- 0.98åçš„æ–°å˜åŠ¨ï¼Œä¹‹å‰ç‰ˆæœ¬æ²¡æœ‰.port,é»˜è®¤ç«¯å£ä¸º60000 -->
	<property>
		<name>hbase.master.port</name>
		<value>16000</value>
	</property>

	<property>   
		<name>hbase.zookeeper.quorum</name>
	     <value>tsingdata01,tsingdata02,tsingdata03</value>
	</property>

	<property>   
		<name>hbase.zookeeper.property.dataDir</name>
	     <value>/opt/module/zookeeper-3.5.7/zkData</value>
	</property>
  

</configuration>
```

3ï¼‰é…ç½®**regionservers**ï¼Œä¿®æ”¹ä¸º

```
tsingdata01
tsingdata02
tsingdata03
```

4ï¼‰è½¯è¿æ¥hadoopé…ç½®æ–‡ä»¶åˆ°hbaseï¼š

```
ln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-2.0.5/conf/core-site.xml
ln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-2.0.5/conf/hdfs-site.xml
```

5ï¼‰åŒæ­¥åˆ°å…¶ä»–æœåŠ¡å™¨

```
xsync hbase-2.0.5/
```

#### å¯åŠ¨å…³é—­

```
#/etc/profileä¸­æ·»åŠ 

#HBase
export HBASE_HOME=/opt/module/hbase-2.0.5
export PATH=$PATH:$HBASE_HOME/bin
start-hbase.sh
stop-hbase.sh
[root@tsingdata01 bin]# jps
6160 HRegionServer
```

é‡åˆ°çš„é—®é¢˜ï¼šæˆ‘åœ¨å¯åŠ¨hbaseçš„æ—¶å€™ï¼Œå‘ç°HMasterå¼€å¯å‡ ç§’åä¼šæŒ‚æ‰

2021-04-22 10:12:57,420 ERROR [Thread-15] master.HMaster: ***** ABORTING master tsingdata01,16000,1619057573181: Unhandled exception. Starting shutdown. *****

java.net.ConnectException: Call From tsingdata01/192.168.157.128 to tsingdata01:9000 failed on connection exception: java.net.ConnectException: æ‹’ç»è¿æ¥; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

è§£å†³æ–¹æ³•ï¼š

æŸ¥çœ‹æ—¥å¿—åå‘ç°ï¼Œæ˜¯hbase-site.xmlé…ç½®é—®é¢˜ï¼Œhdfsçš„ç«¯å£å·ä¸º8020ï¼Œè€Œä¸æ˜¯9000.



### æŸ¥çœ‹HBaseé¡µé¢

å¯åŠ¨æˆåŠŸåï¼Œå¯ä»¥é€šè¿‡â€œhost:portâ€çš„æ–¹å¼æ¥è®¿é—®HBaseç®¡ç†é¡µé¢ï¼š[http://192.168.28.116:16010](http://192.168.28.116:16010/master-status)

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1619060461470-69cad218-e044-4761-b757-dd9bcdfd567a.png)

# Solr

### Solréƒ¨ç½²

1ï¼‰Solr ç‰ˆæœ¬è¦æ±‚å¿…é¡»æ˜¯ 5.2.1ï¼Œè§å®˜ç½‘

2ï¼‰Solr ä¸‹è½½ï¼šhttp://archive.apache.org/dist/lucene/solr/5.2.1/solr-5.2.1.tgz

3ï¼‰æŠŠ solr-5.2.1.tgz ä¸Šä¼ åˆ° tsingdata01çš„/opt/software ç›®å½•ä¸‹

4ï¼‰è§£å‹ solr-5.2.1.tgz åˆ°/opt/module/ç›®å½•ä¸‹é¢

```
tar -zxvf solr-5.2.1.tgz -C /opt/module/
```

5ï¼‰ä¿®æ”¹ solr-5.2.1 çš„åç§°ä¸º solr

```
mv solr-5.2.1/ solr
```

#### é…ç½®

6ï¼‰è¿›å…¥ solr/bin ç›®å½•ï¼Œä¿®æ”¹ solr.in.sh æ–‡ä»¶

```
vim bin/solr.in.sh
```

æ·»åŠ ä¸‹åˆ—æŒ‡ä»¤

```
ZK_HOST="tsingdata01:2181,tsingdata02:2181,tsingdata03:2181"
SOLR_HOST="tsingdata01"
# Sets the port Solr binds to, default is 8983
# å¯ä¿®æ”¹ç«¯å£å·
SOLR_PORT=8983
```

7ï¼‰åˆ†å‘ Solrï¼Œè¿›è¡Œ Cloud æ¨¡å¼éƒ¨ç½²

```
xsync solr
```

æç¤ºï¼šåˆ†å‘å®Œæˆåï¼Œåˆ†åˆ«å¯¹ **tsingdata02**ã€**tsingdata03**ä¸»æœº/opt/module/solr/bin ä¸‹çš„ solr.in.sh æ–‡ä»¶ï¼Œä¿®æ”¹ä¸º SOLR_HOST=å¯¹åº”ä¸»æœºåã€‚

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618974627086-860c9205-c92c-4243-bcfa-6478dbda447d.png)

#### å¯åŠ¨

```
#/etc/profileä¸­æ·»åŠ 

#Solr
export SOLR_HOME=/opt/module/solr
export PATH=$PATH:$SOLR_HOME/bin
```

8ï¼‰åœ¨ä¸‰å°èŠ‚ç‚¹ä¸Šåˆ†åˆ«å¯åŠ¨ Solrï¼Œè¿™ä¸ªå°±æ˜¯ **Cloud** æ¨¡å¼

```
solr start
solr start
solr start
```

Solr home directory /opt/module/solr must contain a solr.xml file!

è§£å†³

solr start -s /opt/module/solr/server/solr

```
[root@tsingdata01 solr]# solr start
Waiting to see Solr listening on port 8983 [-]  
Started Solr server on port 8983 (pid=6563). Happy searching!
```

æç¤ºï¼šå¯åŠ¨ Solr å‰ï¼Œéœ€è¦æå‰å¯åŠ¨ **Zookeeper** æœåŠ¡ã€‚

### æŸ¥çœ‹Solré¡µé¢

9ï¼‰Web è®¿é—® 8983 ç«¯å£ï¼Œå¯æŒ‡å®šä¸‰å°èŠ‚ç‚¹ä¸­çš„ä»»æ„ä¸€å° IPï¼Œhttp://192.168.28.116:8983/

æç¤ºï¼šUI ç•Œé¢å‡ºç° **Cloud** èœå•æ æ—¶ï¼ŒSolr çš„ Cloud æ¨¡å¼æ‰ç®—éƒ¨ç½²æˆåŠŸã€‚

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618974787596-d7e653e2-de75-4ff8-b12d-683c5f992dea.png)

# ğŸ¾Atlas

1ã€æŠŠ apache-atlas-2.0.0-server.tar.gz  ä¸Šä¼ åˆ° tsingdata01çš„/opt/software ç›®å½•ä¸‹

2ã€è§£å‹ apache-atlas-2.0.0-server.tar.gz  åˆ°/opt/module/ç›®å½•ä¸‹é¢

```
cd /opt/software
tar -zxvf apache-atlas-2.0.0-server.tar.gz -C /opt/module/
```

3ã€ä¿®æ”¹ apache-atlas-2.0.0 çš„åç§°ä¸º atlas

```
cd /opt/module
mv apache-atlas-2.0.0/ atlas
```

## ğŸ’Atlasé›†æˆå¤–éƒ¨æ¡†æ¶

### Atlas é›†æˆ Hbase

 

1ï¼‰è¿›å…¥/opt/module/atlas/conf/ç›®å½•ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶

```
vim atlas-application.properties
```



```
#ä¿®æ”¹atlaså­˜å‚¨æ•°æ®ä¸»æœº
atlas.graph.storage.hostname=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```

2ï¼‰è¿›å…¥åˆ°/opt/module/atlas/conf/hbase è·¯å¾„ï¼Œæ·»åŠ  Hbase é›†ç¾¤çš„é…ç½®æ–‡ä»¶åˆ°${Atlas_Home} 

```
ln -s /opt/module/hbase-2.0.5/conf/ /opt/module/atlas/conf/hbase/conf
```

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618987130418-87e6cb57-375b-4330-a506-9731138f8736.png)

3ï¼‰åœ¨/opt/module/atlas/conf/atlas-env.sh ä¸­æ·»åŠ  HBASE_CONF_DIR

```
vim atlas-env.sh
#æ·»åŠ  HBase é…ç½®æ–‡ä»¶è·¯å¾„
export HBASE_CONF_DIR=/opt/module/atlas/conf/hbase/conf
```

### Atlas é›†æˆ Solr



1ï¼‰è¿›å…¥/opt/module/atlas/conf ç›®å½•ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶

```
vim atlas-application.properties
#ä¿®æ”¹å¦‚ä¸‹é…ç½®
atlas.graph.index.search.solr.zookeeper-url=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```

2ï¼‰å°† Atlas è‡ªå¸¦çš„ Solr æ–‡ä»¶å¤¹æ‹·è´åˆ°å¤–éƒ¨ Solr é›†ç¾¤çš„å„ä¸ªèŠ‚ç‚¹ã€‚

```
cp -r /opt/module/atlas/conf/solr /opt/module/solr/
```

3ï¼‰è¿›å…¥åˆ°/opt/module/solr è·¯å¾„ï¼Œä¿®æ”¹æ‹·è´è¿‡æ¥çš„é…ç½®æ–‡ä»¶åç§°ä¸º atlas_conf

```
mv solr atlas_conf
```

4ï¼‰åœ¨ Cloud æ¨¡å¼ä¸‹ï¼Œå¯åŠ¨ Solrï¼ˆéœ€è¦æå‰å¯åŠ¨ Zookeeper é›†ç¾¤ï¼‰ï¼Œå¹¶åˆ›å»º collection

```
cd /opt/module/solr
bin/solr create -c vertex_index -d /opt/module/solr/atlas_conf -shards 3 -replicationFactor 2 
bin/solr create -c edge_index -d /opt/module/solr/atlas_conf -shards 3 -replicationFactor 2 
bin/solr create -c fulltext_index -d /opt/module/solr/atlas_conf -shards 3 -replicationFactor 2
```

-shards 3ï¼šè¡¨ç¤ºè¯¥é›†åˆåˆ†ç‰‡æ•°ä¸º 3

-replicationFactor 2ï¼šè¡¨ç¤ºæ¯ä¸ªåˆ†ç‰‡æ•°éƒ½æœ‰ 2 ä¸ªå¤‡ä»½

vertex_indexã€edge_indexã€fulltext_indexï¼šè¡¨ç¤ºé›†åˆåç§°



æ³¨æ„ï¼šå¦‚æœéœ€è¦åˆ é™¤ vertex_indexã€edge_indexã€fulltext_index ç­‰ collection å¯ä»¥æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ã€‚

```
bin/solr delete -c vertex_index
bin/solr delete -c edge_index
bin/solr delete -c fulltext_index
```

5ï¼‰éªŒè¯åˆ›å»º collection æˆåŠŸ

ç™»å½• solr web æ§åˆ¶å°ï¼š[http://192.168.28.116:8983](http://192.168.157.128:8983/solr/#/~cloud) çœ‹åˆ°å¦‚ä¸‹å›¾æ˜¾ç¤ºï¼š

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618987962292-a3ded092-1398-44f9-a2d5-56741be40e5d.png)

### Atlas é›†æˆ Kafka



1ï¼‰è¿›å…¥/opt/module/atlas/conf/ç›®å½•ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶ atlas-application.properties

```
vim atlas-application.properties
```



```
#########	Notification Configs	#########

atlas.notification.embedded=false

atlas.kafka.data=/opt/module/kafka/logs

atlas.kafka.zookeeper.connect=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181/kafka

atlas.kafka.bootstrap.servers=tsingdata01:9092,tsingdata02:9092,tsingdata03:9092
atlas.kafka.zookeeper.session.timeout.ms=4000 
atlas.kafka.zookeeper.connection.timeout.ms=2000

atlas.kafka.enable.auto.commit=true
```

2ï¼‰å¯åŠ¨ Kafka é›†ç¾¤ï¼Œå¹¶åˆ›å»º Topic

```
cd /opt/module/kafka
bin/kafka-topics.sh	--zookeeper tsingdata01:2181,tsingdata02:2181,tsingdata03:2181 --create --replication-factor 3 --partitions 3 --topic _HOATLASOK
bin/kafka-topics.sh	--zookeeper tsingdata01:2181,tsingdata02:2181,tsingdata03:2181 --create --replication-factor 3 --partitions 3 --topic ATLAS_ENTITIES
```



### Atlas å…¶ä»–é…ç½®



1ï¼‰è¿›å…¥/opt/module/atlas/conf/ç›®å½•ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶ atlas-application.properties

```
vim atlas-application.properties
######### Server Properties ######### 
atlas.rest.address=http://tsingdata01:21000
#If enabled and set to true, this will run setup steps when the server starts
atlas.server.run.setup.on.start=false

#########	Entity Audit Configs	#########
atlas.audit.hbase.zookeeper.quorum=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```



2ï¼‰è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼Œè¿›å…¥/opt/module/atlas/conf/è·¯å¾„ï¼Œä¿®æ”¹å½“å‰ç›®å½•ä¸‹çš„ atlas-log4j.xml 

```
vim atlas-log4j.xml
#å»æ‰å¦‚ä¸‹ä»£ç çš„æ³¨é‡Š

<appender name="perf_appender" class="org.apache.log4j.DailyRollingFileAppender">
  <param name="file" value="${atlas.log.dir}/atlas_perf.log" /> <param name="datePattern" value="'.'yyyy-MM-dd" /> <param name="append" value="true" />
  <layout class="org.apache.log4j.PatternLayout">
  <param name="ConversionPattern" value="%d|%t|%m%n" /> </layout>
</appender>

<logger name="org.apache.atlas.perf" additivity="false"> <level value="debug" />
	<appender-ref ref="perf_appender" />
</logger>
```

### Atlas é›†æˆ Hive

1ï¼‰è¿›å…¥/opt/module/atlas/conf/ç›®å½•ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶ atlas-application.properties

```
vim atlas-application.properties
######### Hive Hook Configs ç›´æ¥æ·»åŠ #######
atlas.hook.hive.synchronous=false
atlas.hook.hive.numRetries=3
atlas.hook.hive.queueSize=10000
atlas.cluster.name=primary
```

2ï¼‰è§£å‹ apache-atlas-2.0.0-hive-hook.tar.gz åˆ°/opt/module/

```
tar -zxvf apache-atlas-2.0.0-hive-hook.tar.gz -C /opt/module/
```

3ï¼‰å‰ªåˆ‡ hook å’Œ hook-bin ç›®å½•åˆ°åˆ°/opt/module/atlas æ–‡ä»¶å¤¹ä¸­

```
mv /opt/module/apache-atlas-hive-hook-2.0.0/hook-bin/ /opt/module/atlas/ 
mv /opt/module/apache-atlas-hive-hook-2.0.0/hook /opt/module/atlas/
```

4ï¼‰å°† atlas-application.properties é…ç½®æ–‡ä»¶åŠ å…¥åˆ° atlas-plugin-classloader-1.0.0.jar ä¸­

```
zip -u /opt/module/atlas/hook/hive/atlas-plugin-classloader-2.0.0.jar /opt/module/atlas/conf/atlas-application.properties
cp /opt/module/atlas/conf/atlas-application.properties /opt/module/hive/conf/
```

åŸå› ï¼šè¿™ä¸ªé…ç½®ä¸èƒ½å‚ç…§å®˜ç½‘ï¼Œå°†é…ç½®æ–‡ä»¶è€ƒåˆ° hive çš„ conf ä¸­ã€‚å‚è€ƒå®˜ç½‘çš„åšæ³•ä¸€ç›´è¯»å–ä¸åˆ° atlas-application.properties é…ç½®æ–‡ä»¶ï¼Œçœ‹äº†æºç å‘ç°æ˜¯åœ¨ classpath è¯»å–çš„è¿™ä¸ªé…ç½®æ–‡ä»¶ï¼Œæ‰€ä»¥å°†å®ƒå‹åˆ° jar é‡Œé¢ã€‚

5ï¼‰åœ¨/opt/module/hive/conf/hive-site.xml æ–‡ä»¶ä¸­è®¾ç½® Atlas hook

```
vim hive-site.xml
<property>
  <name>hive.exec.post.hooks</name>
  <value>org.apache.atlas.hive.hook.HiveHook</value>
</property>
vim hive-env.sh
#åœ¨ tez å¼•æ“ä¾èµ–çš„ jar åŒ…åé¢è¿½åŠ  hive æ’ä»¶ç›¸å…³ jar åŒ…
export HIVE_AUX_JARS_PATH=/opt/module/atlas/hook/hive
```

## ğŸš¢å¿«é€Ÿå¯åŠ¨

```
hdp start
zk.sh start
kf.sh start
hbase.sh start
solr.sh start
```

è¿›å…¥/opt/module/atlas è·¯å¾„ï¼Œé‡æ–°å¯åŠ¨ **Atlas** æœåŠ¡

```
cd /opt/module/atlas
bin/atlas_start.py
```

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618994732853-c0aa2e64-ab00-4d8f-b33c-d2a845952cca.png)

æç¤ºï¼šé”™è¯¯ä¿¡æ¯æŸ¥çœ‹è·¯å¾„ï¼š/opt/module/atlas/logs/*.out å’Œ application.log



æˆ‘é‡åˆ°çš„é—®é¢˜1ï¼šè™½ç„¶æç¤ºæœåŠ¡å¯åŠ¨ï¼Œä½†æ˜¯æ— æ³•è®¿é—®http://192.168.28.116:21000

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618996560524-2cf3a66f-a46a-42a9-ade4-599736a64177.png)

ç„¶åæŸ¥çœ‹application.logæ—¥å¿—ï¼š

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618995952576-ea31aa87-ce70-4735-8035-235223b9bf16.png)

æ£€æŸ¥åå‘ç°æ˜¯atlas.graph.storage.hostname=å¤„å¤šæ·»åŠ äº†ä¸€äº›ï¼Œåˆ æ‰å³å¯ã€‚



é‡åˆ°çš„é—®é¢˜2ï¼š

2021-04-22 12:28:32,338 ERROR - [main:] ~ GraphBackedSearchIndexer.initialize() failed (GraphBackedSearchIndexer:307)

org.apache.solr.common.SolrException: Could not load collection from ZK: vertex_index

å¾ˆæ˜æ˜¾æ˜¯solrçš„é—®é¢˜ï¼Œåˆ æ‰collectionå†åˆ›å»ºå³å¯ã€‚è¯¦è§ä¸Šé¢Atlasé›†æˆSolr

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1619061159521-ed0aeab0-ae0c-424c-9b72-607387c89b64.png)

è®¿é—®åœ°å€ï¼šhttp://192.168.28.116:21000

æ³¨æ„ï¼šç­‰å¾…æ—¶é—´å¤§æ¦‚ 3 åˆ†é’Ÿã€‚å¯åŠ¨éå¸¸æ…¢ï¼ï¼ï¼

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1619069873360-052cdaf5-1b8e-4605-a4ec-87fd34173ebd.png)

```
è´¦æˆ·ï¼šadmin
å¯†ç ï¼šadmin
```

# Kylin

## ç‰ˆæœ¬é€‰å‹

```
3.0.2
```

## å®‰è£…

### å®‰è£…åœ°å€

å®˜ç½‘åœ°å€ï¼šhttp://kylin.apache.org/cn/
å®˜æ–¹æ–‡æ¡£ï¼šhttp://kylin.apache.org/cn/docs/
ä¸‹è½½åœ°å€ï¼šhttp://kylin.apache.org/cn/download/

### å®‰è£…éƒ¨ç½²

1ï¼‰ä¸‹è½½å¹¶ä¸Šä¼ åˆ°/opt/software

2ï¼‰è§£å‹ apache-kylin-3.0.2-bin.tar.gz åˆ° /opt/module

```
tar -zxvf apache-kylin-3.0.2-bin.tar.gz -C /opt/module/

cd /opt/module
mv apache-kylin-3.0.2-bin kylin
```



`æ³¨æ„`ï¼šéœ€è¦åœ¨/etc/profileæ–‡ä»¶ä¸­é…ç½®HADOOP_HOMEï¼ŒHIVE_HOMEï¼ŒHBASE_HOMEå¹¶å°†å…¶å¯¹åº”çš„ sbinï¼ˆå¦‚æœæœ‰è¿™ä¸ªç›®å½•çš„è¯ï¼‰å’Œbinç›®å½•é…ç½®åˆ°Pathï¼Œæœ€åéœ€è¦sourceä½¿å…¶ç”Ÿæ•ˆã€‚

```
vim /etc/profile
```



3ï¼‰é…ç½®ç¯å¢ƒ

```
#hadoop
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin

#hive
export HIVE_HOME=/opt/module/hive
export HIVE_CONF=/opt/module/hive/conf
export PATH=$PATH:$HIVE_HOME/bin

#hbase
export HBASE_HOME=/opt/module/hbase-2.0.5
export PATH=$PATH:$HBASE_HOME/bin

#kylin
export KYLIN_HOME=/opt/module/kylin
export PATH=$PATH:$KYLIN_HOME/bin

#sparkç›®å½•
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```



```
source /etc/profile
```



4ï¼‰å…¼å®¹æ€§é—®é¢˜

ç”±äº/opt/module/spark/jarsä¸­çš„hiveä¾èµ–ç‰ˆæœ¬ä¸æˆ‘ä»¬ä½¿ç”¨çš„hiveå†²çª

```
[root@tsingdata01 jars]# ls -al | grep hive
-rw-r--r--.  1 tsing-data tsing-data   138464 9æœˆ   8 2020 hive-beeline-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data    40817 9æœˆ   8 2020 hive-cli-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data 11498852 9æœˆ   8 2020 hive-exec-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data   100680 9æœˆ   8 2020 hive-jdbc-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data  5505200 9æœˆ   8 2020 hive-metastore-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data  1565700 9æœˆ   8 2020 orc-core-1.5.5-nohive.jar
-rw-r--r--.  1 tsing-data tsing-data   812313 9æœˆ   8 2020 orc-mapreduce-1.5.5-nohive.jar
-rw-r--r--.  1 tsing-data tsing-data  1358996 9æœˆ   8 2020 spark-hive_2.11-2.4.7.jar
-rw-r--r--.  1 tsing-data tsing-data  1815976 9æœˆ   8 2020 spark-hive-thriftserver_2.11-2.4.7.jar
```

éœ€è¦é€šè¿‡ä¿®æ”¹è„šæœ¬æ’é™¤ä¾èµ–å†²çª

```
cd /opt/module/kylin/bin
vim find-hive-dependency.sh
hive_lib=`find -L ${hive_lib_dir} -name '*.jar' ! -name '*druid*' ! -name '*jackson*' ! -name '*metastore*' !-name '*slf4j*' ! -name '*avatica*' ! -name '*calcite*' ! -name '*jackson-datatype-joda*' ! -name '*derby*' -printf '%p:' | sed 's/:$//'`
vim find-spark-dependency.sh
spark_dependency=`find -L $spark_home/jars -name '*.jar' ! -name '*jackson*' ! -name '*metastore*' ! -name '*slf4j*' ! -name '*calcite*' ! -name '*doc*' ! -name '*test*' ! -name '*sources*' ''-printf '%p:' | sed 's/:$//'`
```

å¦‚æœæ˜¯ç¬¬äºŒæ¬¡å¯åŠ¨ï¼Œå…ˆåˆ é™¤ç¼“å­˜

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1620874122779-90a8ebae-2e10-4e00-94d7-5e07b0f322d6.png)

5ï¼‰å¯åŠ¨

å¯åŠ¨Kylinä¹‹å‰ï¼Œéœ€å…ˆå¯åŠ¨Hadoopï¼ˆhdfs, yarn, jobhistoryserverï¼‰ã€Zookeeperã€Hbase

```
# å¯åŠ¨hadoop
hdp start

# å¯åŠ¨zk
zk.sh start

# å¯åŠ¨hbase
start-hbase.sh

# hive
nohup hive --service metastore &
hive

# å¯åŠ¨
kylin.sh start
```

è¯´æ˜å¯åŠ¨æˆåŠŸäº†

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1620874167724-0f9a2e69-6f19-41d5-9297-92d377e476f8.png)

å¯åŠ¨ä¹‹åæŸ¥çœ‹å„ä¸ªèŠ‚ç‚¹è¿›ç¨‹ï¼š

```
xcall jps
```



```
--------------------- tsingdata01 ----------------
3360 JobHistoryServer(MRçš„å†å²æœåŠ¡ï¼Œå¿…é¡»å¯åŠ¨)
31425 HMaster
3282 NodeManager
3026 DataNode
53283 Jps
2886 NameNode
44007 RunJar
2728 QuorumPeerMain
31566 HRegionServer
--------------------- tsingdata02 ----------------
5040 HMaster
2864 ResourceManager
9729 Jps
2657 QuorumPeerMain
4946 HRegionServer
2979 NodeManager
2727 DataNode
--------------------- tsingdata03 ----------------
4688 HRegionServer
2900 NodeManager
9848 Jps
2636 QuorumPeerMain
2700 DataNode
2815 SecondaryNameNode
```



è®¿é—®ï¼šhttp://192.168.28.116:7070/kylin

```
ç”¨æˆ·å	ADMIN
å¯†ç 	KYLIN
```



6ï¼‰å…³é—­

```
bin/kylin.sh stop
```





# Superset

## å‰ç½®è¦æ±‚



```
- Python3
- Anaconda/miniconda
```

### å®‰è£…Anaconda

https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/

ä¸‹è½½ä¸Šä¼ åˆ°tsingdata02çš„/opt/software

```
bash Miniconda3-latest-Linux-x86_64.sh
```



ä¸€è·¯å›è½¦ã€yes

![img](https://cdn.nlark.com/yuque/0/2020/png/519413/1599734808927-b3da7918-4a09-4679-9239-e59c04ff8064.png)

é»˜è®¤è·¯å¾„ï¼š/root/miniconda3

æˆ‘ä»¬è¦ä¿®æ”¹ä¸º  /opt/module/miniconda3

![img](https://cdn.nlark.com/yuque/0/2020/png/519413/1599734891859-907e3bb9-2c43-41a4-9034-de1b6c717f90.png)

å®‰è£…å®Œæ¯•ï¼

æ·»åŠ ä¸ºæœ€æ–°minicondaè·¯å¾„

```
vim ~/.bashrc
export PATH="/opt/module/miniconda3/bin:$PATH"
source ~/.bashrc
```

é‡æ–°æ‰“å¼€ï¼Œbaseä¸ºé»˜è®¤ç¯å¢ƒ  3.9.1 æ˜¯é»˜è®¤çš„pythonç‰ˆæœ¬

```
(base) [root@tsingdata01 software]# python
Python 3.9.1 (default, Dec 11 2020, 14:32:07) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
```

æ˜¯å¦æ¯æ¬¡ç™»é™†è‡ªåŠ¨æ¿€æ´»baseç¯å¢ƒï¼Œ

```
conda config --set auto_activate_base false
```

å†æ¬¡å¼€å¯å°±ä¸ä¼šæœ‰baseäº†

æ¿€æ´»baseç­‰ç¯å¢ƒ

```
conda activate éœ€è¦æ¿€æ´»çš„ç¯å¢ƒï¼Œå¦‚base
```

## å®‰è£…Superset

### ç¯å¢ƒ

ä½¿ç”¨pipæˆ–è€…condaï¼ˆPythonçš„åŒ…ç®¡ç†å·¥å…·ï¼‰ï¼Œå’Œcentosçš„yumç±»ä¼¼ã€‚

é…ç½®condaå›½å†…é•œåƒ



```
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
```



æ˜¾ç¤ºä¸‹è½½åœ°å€ï¼š

```
conda config --set show_channel_urls yes
```

åˆ›å»ºpython3.6ç¯å¢ƒï¼ˆä¸åˆ›å»ºè€Œä½¿ç”¨é»˜è®¤çš„ä¹Ÿå¯ä»¥ï¼‰

```
conda create -name superset python=3.6  # é€€å‡ºbaseç¯å¢ƒï¼Œæˆ–è€…é‡å¯å†æ‰§è¡Œè¿™ä¸ªå‘½ä»¤
[root@tsingdata01 ~]# conda activate superset
(superset) [root@tsingdata01 ~]#
```

condaç¯å¢ƒç®¡ç†å‘½ä»¤

```
1. åˆ›å»ºç¯å¢ƒï¼šconda create -n env_name

2. æŸ¥çœ‹æ‰€æœ‰ç¯å¢ƒï¼šconda info -envs

3. åˆ é™¤ç¯å¢ƒ: conda remove -n env_name --all
```

æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ

```
conda activate
```

å…³é—­è™šæ‹Ÿç¯å¢ƒ

```
conda deactivate
```

### éƒ¨ç½²Superset

å®˜ç½‘ [http://superset.apache.org/installation.html](http://superset.apache.org/installation.html#getting-started)

å®‰è£…python å’Œ å…¶ä»–ä¾èµ–

```
yum install -y python-setuptools
yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel
```



`é‡åˆ°çš„é—®é¢˜`ï¼š-bash: /usr/bin/yum: /usr/bin/python: åçš„è§£é‡Šå™¨: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•

`è§£å†³æ–¹æ³•`ï¼š

ä¿®æ”¹ä»¥ä¸‹é…ç½®æ–‡ä»¶

```
vim /usr/bin/yum
vim /usr/libexec/urlgrabber-ext-down
#!/usr/bin/python2 -> #!/usr/bin/python2.7
```

setuptools pipå‡çº§è‡³æœ€æ–°ç‰ˆ

```
pip install --upgrade setuptools pip -i https://pypi.douban.com/simple
```

å®‰è£…Superset

```
pip install apache-superset -i https://pypi.douban.com/simple
```

åˆå§‹åŒ–supersetæ•°æ®åº“

```
superset db upgrade
```

åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·

```
export FLASK_APP=superset
flask fab create-admin
```



```
[root@tsingdata02 ~]# flask fab create-admin
Username [admin]: tsingdata
User first name [admin]: 
User last name [user]: 
Email [admin@fab.org]: 
Password: 12345678
Repeat for confirmation: 
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
/opt/module/miniconda3/lib/python3.9/site-packages/flask_caching/__init__.py:201: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
  warnings.warn(
No PIL installation found
INFO:superset.utils.screenshots:No PIL installation found
Recognized Database Authentications.
Admin User tsingdata created.
```

åˆå§‹åŒ–

```
superset init
```

å®‰è£…gunicornï¼Œè¿™æ˜¯ç±»ä¼¼tomcatçš„python webæœåŠ¡å™¨

```
pip install gunicorn -i https://pypi.douban.com/simple
```



**å¯åŠ¨**



```
gunicorn --workers 5 --timeout 120 --bind tsingdata01:8787 "superset.app:create_app()" --daemon
```



- workersï¼šæŒ‡å®šè¿›ç¨‹ä¸ªæ•°
- timeoutï¼š worker è¿›ç¨‹è¶…æ—¶æ—¶é—´ï¼Œè¶…æ—¶ä¼šè‡ªåŠ¨é‡å¯

- bindï¼šç»‘å®šæœ¬æœºåœ°å€ï¼Œå³ä¸ºSupersetè®¿é—®åœ°å€
- daemonï¼šåå°è¿è¡Œ4



è®¿é—®ï¼šhttp://192.168.28.116:8787/

```
ç”¨æˆ·å tsingdata
å¯†ç 	12345678
```

åœæ­¢

```
ps -ef | awk '/superset/ && !/awk/{print $2}' | xargs kill -9
```

