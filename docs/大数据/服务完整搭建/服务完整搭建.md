# 服务规划

在部署组件之前，请确认当前磁盘空间是否足够！

| 服务名称     | 版本  | 功能                                        | 子服务                | 服务器      | 服务器      | 服务器      |
| ------------ | ----- | ------------------------------------------- | --------------------- | ----------- | ----------- | ----------- |
|              |       |                                             |                       | tsingdata01 | tsingdata02 | tsingdata03 |
| Hadoop——HDFS | 3.1.3 | 使用Hive的前提                              | NameNode              | √           |             |             |
|              |       |                                             | DataNode              | √           | √           | √           |
|              |       |                                             | SecondaryNameNode     |             |             | √           |
| Hadoop——Yarn | 3.1.3 |                                             | NodeManager           | √           | √           | √           |
|              |       |                                             | Resourcemanager       |             | √           |             |
|              |       |                                             |                       |             |             |             |
| Zookeeper    | 3.5.7 | Kafka的前提，存储了Broker信息和消费者信息。 | QuorumPeerMain        | √           | √           | √           |
| Kafka        | 2.4.1 | 读取元数据管理                              | Kafka                 | √           | √           | √           |
| HBase        | 2.0.5 | 存储元数据                                  | HMaster               | √           |             |             |
|              |       |                                             | HRegionServer         | √           | √           | √           |
| Solr         | 5.2.1 | 检索、查询元数据                            | Jar                   | √           | √           | √           |
| Hive         | 3.1.2 |                                             | Hive                  | √           |             |             |
| MySQL        | 5.7   | 存储Hive的元数据                            | MySQL                 | √           |             |             |
| Azkaban      | 3.8.4 | 调度                                        | AzkabanWebServer      | √           |             |             |
|              |       |                                             | AzkabanExecutorServer | √           |             |             |
| Atlas        | 2.0   |                                             | Atlas                 | √           |             |             |
| 服务数总计   |       |                                             |                       | 13          | 7           | 7           |

# Java8

```
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
vim /etc/profile
# Java
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JRE_HOME=$JAVA_HOME/jre
source /etc/profile
[root@hadoop01 jdk1.8.0_212]# java -version
java version "1.8.0_212"
Java(TM) SE Runtime Environment (build 1.8.0_212-b10)
Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)
```

# DataX

https://github.com/alibaba/DataX

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621494982555-d04ab997-5d04-4117-842a-ef57e91ecc1a.png)

解压安装

示例

```
python2 bin/datax.py job/job.json
```

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621496393295-329a3d30-69ec-45cb-86f5-bda302f0e385.png)

模板

```
python2 bin/datax.py -r streamreader -w streamwriter\
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "streamreader", 
                    "parameter": {
                        "column": [], 
                        "sliceRecordCount": ""
                    }
                }, 
                "writer": {
                    "name": "streamwriter", 
                    "parameter": {
                        "encoding": "", 
                        "print": true
                    }
                }
            }
        ], 
        "setting": {
            "speed": {
                "channel": ""
            }
        }
    }
}
```

在job下创建stream2stream.json

```
{
  "job": {
    "content": [
      {
        "reader": {
          "name": "streamreader",
          "parameter": {
            "sliceRecordCount": 10,
            "column": [
              {
                "type": "long",
                "value": "10"
              },
              {
                "type": "string",
                "value": "hello，你好，清数研究院"
              }
            ]
          }
        },
        "writer": {
          "name": "streamwriter",
          "parameter": {
            "encoding": "UTF-8",
            "print": true
          }
        }
      }
    ],
    "setting": {
      "speed": {
        "channel": 5
       }
    }
  }
}
python2 bin/datax.py job/stream2stream.json
```



## MySQL数据源

在mysql创建表，并添加

```
create database test;
use test;
create table stu(id int, name varchar(255));

insert into stu(id, name)
values
(1, 'zhangsan'),
(2, 'lisi'),
(3, 'wangwu');
vim mysql2hdfs.json
{
    "job": {
        "setting": {
            "speed": {
                 "channel": 3
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "root",
                        "column": [
                            "id",
                            "name"
                        ],
                        "connection": [
                            {
                                "table": [
                                    "stu"
                                ],
                                "jdbcUrl": [
     "jdbc:mysql://127.0.0.1:3306/test"
                                ]
                            }
                        ]
                    }
                },
 "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://tsingdata01:8020",
                        "fileType": "text",
                        "path": "/",
                        "fileName": "student.txt",
                        "column": [
                            {
                                "name": "id",
                                "type": "int"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                        ],
                        "writeMode": "append",
                        "fieldDelimiter": "\t",
                    }
                }
            }
        ]
    }
}
```



```
python2 bin/datax.py job/mysql2hdfs.json
```

http://192.168.28.116:9870/explorer.html#/

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621504220044-a61aa850-0bde-433f-87ba-9051aaa0b391.png)

## DataX-web可视化

https://github.com/WeiYe-Jing/datax-web

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1621495031834-d0e8af70-31aa-4b79-8001-a5ead7d76af5.png)

安装教程：https://github.com/WeiYe-Jing/datax-web/blob/master/doc/datax-web/datax-web-deploy.md



访问：http://192.168.28.116:9527/index.html

# Hadoop

```
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
vim /etc/profile
# Hadoop
export HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source /etc/profile
[root@hadoop01 jdk1.8.0_212]# hadoop version
Hadoop 3.1.3
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579
Compiled by ztang on 2019-09-12T02:47Z
Compiled with protoc 2.5.0
From source with checksum ec785077c385118ac91aadde5ec9799
This command was run using /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-common-3.1.3.jar
```

## 伪分布式启动

### 配置

```
cd /opt/module/hadoop-3.1.3/etc/hadoop
vim core-site.xml
		<property>
        <name>hadoop.tmp.dir</name>
        <value>file:/opt/module/hadoop-3.1.3/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
vim hdfs-site.xml
		<property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/opt/module/hadoop-3.1.3/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/opt/module/hadoop-3.1.3/tmp/dfs/data</value>
		</property>
```

伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 **dfs.namenode.name.dir** 和 **dfs.datanode.data.dir**，否则在接下来的步骤中可能会出错。

#### 配置master和slave连接

运行如下指令，并且一直回车

```
ssh-keygen -t rsa 
The key fingerprint is:
SHA256:W5zT7FkSO/9V5mqjivzM35I+P9z/gtLS+uvZSJ+OXho root@hadoop01
The key's randomart image is:
+---[RSA 2048]----+
|                 |
|                 |
|            .    |
|         . + o   |
|        S = * . o|
|         o o * o.|
|        .  oE+o.o|
|       . +o.B@*+o|
|        o.*@&OO+*|
+----[SHA256]-----+
```

运行如下代码

```
cd /root/.ssh
cat id_rsa.pub >> authorized_keys
```

#### 执行 NameNode 的格式化

配置完成后，执行 NameNode 的格式化

```
cd /opt/module/hadoop
bin/hdfs namenode -format
```

成功的话，会看到 “successfully formatted” 的提示



修改start-dfs.sh，stop-dfs.sh文件，添加

```
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs 
HDFS_NAMENODE_USER=root 
HDFS_SECONDARYNAMENODE_USER=root
```



### 启动

```
start-dfs.sh
```

查看jps

```
(base) [root@glong hadoop]# jps
26804 SecondaryNameNode
26344 NameNode
26506 DataNode
26971 Jps
```

访问http://ip:9870/

### 关闭

```
stop-dfs.sh
```

## 集群启动

### 配置

hadoop共需配置6个地方

```
cd /opt/module/hadoop-3.1.3/etc/hadoop
```



#### hadoop-env.sh

```
vim hadoop-env.sh
```



添加如下配置

```
export JAVA_HOME=/opt/module/jdk1.8.0_212

export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export HDFS_ZKFC_USER=root
export HDFS_JOURNALNODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```



#### core-site.xml

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

 <property>
        <name>fs.defaultFS</name>
        <value>hdfs://tsingdata01:8020</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>tsingdata</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>

</configuration>
```



#### hdfs-site.xml

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>/opt/module/hadoop-3.1.3/data/dfs/nn</value>
</property>
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>tsingdata03:9868</value>
</property>
   <!-- datanode结点被指定要存储数据的本地文件系统路径 -->
<property>
   <name>dfs.datanode.data.dir</name>
   <value>/opt/module/hadoop-3.1.3/data/dfs/dn</value>
</property>

<!-- 测试环境指定HDFS副本的数量1 -->
<property>
    <name>dfs.replication</name>
    <value>3</value>
</property>

</configuration>
```



#### yarn-site.xml



```
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>


<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>tsingdata02</value>
</property>
<property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>

<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
<property>
   <name>yarn.nodemanager.pmem-check-enabled</name>
   <value>false</value>
</property>

<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
<property>
   <name>yarn.nodemanager.vmem-check-enabled</name>
   <value>false</value>
</property>

</configuration>
```



#### mapred-site.xml



```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>tsingdata01:10020</value>
</property>

<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>tsingdata01:19888</value>
</property>

</configuration>
```



#### workers

在`etc/hadoop/workers`种添加

```
tsingdata01
tsingdata02
tsingdata03
```



#### 最后：xsync脚本分发

该脚本在：xsync

分发配置文件到其他机器

```
xsync /opt/module/hadoop/etc/hadoop
```



### 格式化

如果集群是第一次启动，需要格式化 （只要格式化1次就行，一定不要再格式化！！！）NameNode（注意格式化之前，一定要先停止上次启动的所有 namenode 和 datanode 进程，然后再删除 data 和 log 数据）



```
 bin/hdfs namenode -format
```



### 启动

```
start-dfs.sh
```



如果想要使用脚本，请参考**脚本**这一章



# mysql



```
# 查看有没有原始mysql
rpm -qa|grep mysql

# 有就卸载
yum remove mysql-community-server-5.6.36-2.el7.x86_64
rpm -qa | grep -i mysql

# 安装
rpm -ivh 01_mysql-community-common-5.7.16-1.el7.x86_64.rpm
rpm -ivh 02_mysql-community-libs-5.7.16-1.el7.x86_64.rpm
rpm -ivh 03_mysql-community-libs-compat-5.7.16-1.el7.x86_64.rpm 
rpm -ivh 04_mysql-community-client-5.7.16-1.el7.x86_64.rpm
rpm -ivh 05_mysql-community-server-5.7.16-1.el7.x86_64.rpm

......

# 启动mysql
systemctl start mysqld

# 密码
grep 'temporary password' /var/log/mysqld.log
如果找不到，请参考： https://blog.csdn.net/pao___pao/article/details/80118904

# 先登录，然后设置密码
mysql -uroot -pVyrrhBd/E3ij

# mysql要求设置复杂密码，所以要修改密码策略
set password=password("Qs23=zs32");
set global validate_password_length=4;
set global validate_password_policy=0;
set password=password("root");

# 进入mysql库
use mysql
# 查询user表
select user,host from user;
# 修改user.表，把Host表内容修改为%
update user set host="%" where user="root";
# 刷新
flush privileges;
# 退出
quit;
```

# Hive

etc/profile

```
# Hive
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
```

source /etc/profile

解压安装



如果后续删除hive并重新安装，请记得先删除mysql中的metastore数据库。



```
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module
mv apache-hive-3.1.2-bin hive
```



下载`mysql-connector-java-5.1.48.tar.gz`



```
tar -zxvf mysql-connector-java-5.1.48.tar.gz
```



然后将`mysql-connector-java-5.1.48`中的`mysql-connector-java-5.1.48.jar`放入到hive/lib中



```
cp mysql-connector-java-5.1.48.jar /opt/module/hive/lib/

chmod 777 /opt/module/hive/lib/mysql-connector-java-5.1.48.jar
```



```
cd hive/conf
mv hive-env.sh.template hive-env.sh

# 解决日志Jar包冲突，进入opt/module/hive/lib目录←
mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak
```



```
vim hive-env.sh
# 添加
HADOOP_HOME=/opt/module/hadoop-3.1.3
export HIVE_CONF_DIR=/opt/module/hadoop-3.1.3/conf
```



```
vim hive-site.xml
```



特别要注意主机名`tsingdata01`、`mysql用户名`、`mysql密码`，根据你自己实际情况来，一定要在mysql创建`metastore数据库`！



```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    
       <!--hive 的元数据服务, 供spark SQL 使用-->
    <property>
        　　　　<name>hive.metastore.uris</name>
        　　　　<value>thrift://tsingdata01:9083</value>
        　　　　<description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
        　　</property>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://tsingdata01:3306/metastore?useSSL=false</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>root</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>

    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>tsingdata01</value>
    </property>

    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
    
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
</configuration>
```



```
chmod 777 hive-site.xml
```



在 HDFS 上创建Hive所需路径，`/tmp/hive`和`/user/hive/warehouse` 两个目录并修改他们的同组权限可写。

```
hadoop fs -mkdir /tmp/hive
hadoop fs -mkdir -p /user/hive/warehouse
hadoop fs -chmod 777 /tmp/hive
hadoop fs -chmod 777 /user/hive/warehouse
```



http://192.168.157.128:9870/explorer.html#/	查看

mysql创建`metastore`，这个metastore对应了hive-site.xml中的配置

```
create database metastore;

grant all privileges on *.* to 'root'@'localhost' identified by 'root';

#允许用户root从localhost的主机连接到mysql服务器，并使用root作为密码
grant all privileges on *.* to 'root'@'%' identified by 'root';

#root使用root密码从任何主机连接到mysql服务器
flush privileges;
```



查看数据库`metastore`的权限

```
SELECT host,user,Grant_priv,Super_priv FROM mysql.user;
+-----------+-----------+------------+------------+
| host      | user      | Grant_priv | Super_priv |
+-----------+-----------+------------+------------+
| %         | root      | Y          | Y          |
| localhost | mysql.sys | N          | N          |
| localhost | root      | N          | Y          |
+-----------+-----------+------------+------------+
```

下面启动服务器

初始化元数据

```
schematool -initSchema -dbType mysql -verbose
```



成功了会在metestore数据库中建很多表



`报错`：



Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V



`解决方法`：



这是因为当时用的hive2.x，与hadoop3.x不兼容。下面方法不要使用，直接升级到hive3，因为即使解决了该问题，还会有新的问题出现。



删除hive中低版本的`guava-14.0.1.jar`包，将hadoop-3.1.3/share/hadoop/common/lib中的`guava-27.0-jre.jar`复制到hive的lib目录下即可。



![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618882226162-1ea4c5ed-bb3d-4f1b-80b6-7eb599e0f939.png)

```
cd hadoop-3.1.3/share/hadoop/common/lib
cp guava-27.0-jre.jar /opt/module/hive/lib
cd /opt/module/hive/lib
rm -rf guava-14.0.1.jar
```



启动

```
nohup hive --service metastore &
nohup hive --service hiveserver2&	# 如果用datagrip等工具需要开启

hive
```



show databases; `报错`：java.lang.RuntimeException: Error in configuring object



原因是Hive的依赖库中缺少lzo的jar包。

解决办法：



把lzo的jar包放到${HIVE_HOME}/lib目录下。



cp hadoop-lzo-0.4.20.jar ${HIVE_HOME}/lib



介绍一下常用的命令



```
打开默认数据库 
	hive> use default; 
显示 default 数据库中的表 
	hive> show tables; 
创建一张表 
	hive> create table student(id int, name string); 
显示数据库中有几张表
	hive> show tables; 
查看表的结构 
	hive> desc student; 
向表中插入数据 
	hive> insert into student values(1000,"ss"); 
查询表中数据 
	hive> select * from student; 
退出 hive 
	hive> quit;
```

# Zookeeper

### ZK集群部署

#### 解压安装

拷贝Zookeeper安装包到Linux系统下

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618907180565-0daadc7f-533a-45f7-b156-09ad581e75dd.png)

或者下载：wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.5.7/apache-zookeeper-3.5.7-bin.tar.gz

解压到指定目录

```
tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
```

同步/opt/module/zookeeper-3.5.7目录内容到**tsingdata02**、**tsingdata03**

```
cd /opt/module
mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
xsync zookeeper-3.5.7/
```

#### 配置服务器编号

（1）在/opt/module/zookeeper-3.5.7/这个目录下创建zkData

```
mkdir -p zkData
```

（2）在/opt/module/zookeeper-3.5.7/zkData目录下创建一个myid的文件

```
touch myid
```



（3）编辑myid文件

注意：添加myid文件，注意一定要在linux里面创建，在notepad++等编辑器里面很可能**乱码。**

```
vim myid
```

​	在文件中添加与server对应的编号：

```
1
```

（4）拷贝配置好的zookeeper到其他机器上

```
xsync myid
```

并分别在**tsingdata02**、**tsingdata03**上修改myid文件中内容为**2**、**3**。



#### 配置修改

```
cd /opt/module/zookeeper-3.5.7/bin
vim zkEnv.sh
```

\# 添加

JAVA_HOME="/opt/module/jdk1.8.0_212"

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618973649437-678d25a7-e3ba-412b-8202-527eaefe4d38.png)

xsync zkEnv.sh

将/opt/module/zookeeper-3.5.7/conf这个路径下的zoo_sample.cfg修改为zoo.cfg；

```
mv zoo_sample.cfg zoo.cfg
```

打开zoo.cfg文件，修改dataDir路径：

```
vim zoo.cfg
```

修改如下内容

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618910571869-0a7643c4-a434-4624-832a-1ca2b75963c1.png)

```
dataDir=/opt/module/zookeeper-3.5.7/zkData
dataLogDir=/opt/module/zookeeper-3.5.7/logs
```

并且在末尾增加如下配置

```
#######################cluster##########################
server.1=tsingdata01:2888:3888
server.2=tsingdata02:2888:3888
server.3=tsingdata03:2888:3888
```

配置参数解读：

**server.A=B:C:D**

其中：

**A**是一个数字，表示这个是第几号服务器；

集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。

**B**是这个服务器的ip地址；

**C**是这个服务器与集群中的Leader服务器交换信息的端口；

**D**是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。



同步zoo.cfg配置文件

```
xsync zoo.cfg
```

####  操作Zookeeper

（1）启动Zookeeper

```
bin/zkServer.sh start
```

（2）查看进程是否启动

jps

4020 Jps

4001 QuorumPeerMain

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618910690164-9073de78-8c4e-411f-ae6f-6da87bd71c5f.png)

（3）查看状态：

```
bin/zkServer.sh status
```

ZooKeeper JMX enabled by default

Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg

Client port found: 2181. Client address: localhost.

（4）启动客户端：

```
bin/zkCli.sh
```

（5）退出客户端：

```
quit
```

（6）停止Zookeeper

```
bin/zkServer.sh stop
```



# Kafka

### Kafka环境准备

jar包下载：http://kafka.apache.org/downloads.html

### Kafka集群部署

上传安装包到在/opt/software处

解压安装包

```
tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/
```

修改解压后的文件名称

```
cd /opt/module
mv kafka_2.11-2.4.1/ kafka
```

在/opt/module/kafka目录下创建logs文件夹

```
mkdir logs
```

#### 配置修改

```
cd config/
vim server.properties
```

修改以下内容（重点修改下面两个图的内容）：

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618910939603-c0a9561f-3961-4291-941f-85f2e66976fa.png)

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618911029670-bc80e172-f23a-4913-a1aa-a8c32912ab21.png)

```
#broker的全局唯一编号，不能重复
broker.id=0
#删除topic功能使能
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的现成数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志存放的路径 
log.dirs=/opt/module/kafka/logs
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接Zookeeper集群地址
zookeeper.connect=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```

配置环境变量

vim /etc/profile

```
#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```

source /etc/profile

分发安装包

xsync kafka/

注意：分发之后记得配置其他机器的环境变量

分别在tsingdata02和tsingdata03上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618911278811-e8a89b1b-eedf-46a6-9993-fa9d54c5cc8b.png)

注意：broker.id不得重复

#### 启动

启动集群（脚本启动见本文档最后**脚本**这一章节）

依次在tsingdata01、tsingdata02、tsingdata03节点上启动kafka

```
bin/kafka-server-start.sh config/server.properties &
```

依次在tsingdata01、tsingdata02、tsingdata03节点上关闭kafka（脚本关闭见本文档的**脚本**这一章节）

```
bin/kafka-server-stop.sh stop
```

# HBase

### HBase部署

上传/opt/software

解压HBase到指定目录：

```
tar -zxvf hbase-2.0.5-bin.tar.gz -C /opt/module
```

#### HBase的配置文件

修改HBase对应的配置文件。

1）/opt/module/hbase-2.0.5/conf/hbase-env.sh添加内容：

```
export JAVA_HOME=/opt/module/jdk1.8.0_212
export HBASE_MANAGES_ZK=false
```

2）hbase-site.xml修改内容：



```
<configuration>
	<property>     
		<name>hbase.rootdir</name>     
		<value>hdfs://tsingdata01:8020/hbase</value>   
	</property>

     <!-- 配置启用集群分布式运行 -->
	<property>   
		<name>hbase.cluster.distributed</name>
		<value>true</value>
	</property>

   <!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 -->
	<property>
		<name>hbase.master.port</name>
		<value>16000</value>
	</property>

	<property>   
		<name>hbase.zookeeper.quorum</name>
	     <value>tsingdata01,tsingdata02,tsingdata03</value>
	</property>

	<property>   
		<name>hbase.zookeeper.property.dataDir</name>
	     <value>/opt/module/zookeeper-3.5.7/zkData</value>
	</property>
  

</configuration>
```

3）配置**regionservers**，修改为

```
tsingdata01
tsingdata02
tsingdata03
```

4）软连接hadoop配置文件到hbase：

```
ln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-2.0.5/conf/core-site.xml
ln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-2.0.5/conf/hdfs-site.xml
```

5）同步到其他服务器

```
xsync hbase-2.0.5/
```

#### 启动关闭

```
#/etc/profile中添加

#HBase
export HBASE_HOME=/opt/module/hbase-2.0.5
export PATH=$PATH:$HBASE_HOME/bin
start-hbase.sh
stop-hbase.sh
[root@tsingdata01 bin]# jps
6160 HRegionServer
```

遇到的问题：我在启动hbase的时候，发现HMaster开启几秒后会挂掉

2021-04-22 10:12:57,420 ERROR [Thread-15] master.HMaster: ***** ABORTING master tsingdata01,16000,1619057573181: Unhandled exception. Starting shutdown. *****

java.net.ConnectException: Call From tsingdata01/192.168.157.128 to tsingdata01:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

解决方法：

查看日志后发现，是hbase-site.xml配置问题，hdfs的端口号为8020，而不是9000.



### 查看HBase页面

启动成功后，可以通过“host:port”的方式来访问HBase管理页面：[http://192.168.28.116:16010](http://192.168.28.116:16010/master-status)

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1619060461470-69cad218-e044-4761-b757-dd9bcdfd567a.png)

# Solr

### Solr部署

1）Solr 版本要求必须是 5.2.1，见官网

2）Solr 下载：http://archive.apache.org/dist/lucene/solr/5.2.1/solr-5.2.1.tgz

3）把 solr-5.2.1.tgz 上传到 tsingdata01的/opt/software 目录下

4）解压 solr-5.2.1.tgz 到/opt/module/目录下面

```
tar -zxvf solr-5.2.1.tgz -C /opt/module/
```

5）修改 solr-5.2.1 的名称为 solr

```
mv solr-5.2.1/ solr
```

#### 配置

6）进入 solr/bin 目录，修改 solr.in.sh 文件

```
vim bin/solr.in.sh
```

添加下列指令

```
ZK_HOST="tsingdata01:2181,tsingdata02:2181,tsingdata03:2181"
SOLR_HOST="tsingdata01"
# Sets the port Solr binds to, default is 8983
# 可修改端口号
SOLR_PORT=8983
```

7）分发 Solr，进行 Cloud 模式部署

```
xsync solr
```

提示：分发完成后，分别对 **tsingdata02**、**tsingdata03**主机/opt/module/solr/bin 下的 solr.in.sh 文件，修改为 SOLR_HOST=对应主机名。

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618974627086-860c9205-c92c-4243-bcfa-6478dbda447d.png)

#### 启动

```
#/etc/profile中添加

#Solr
export SOLR_HOME=/opt/module/solr
export PATH=$PATH:$SOLR_HOME/bin
```

8）在三台节点上分别启动 Solr，这个就是 **Cloud** 模式

```
solr start
solr start
solr start
```

Solr home directory /opt/module/solr must contain a solr.xml file!

解决

solr start -s /opt/module/solr/server/solr

```
[root@tsingdata01 solr]# solr start
Waiting to see Solr listening on port 8983 [-]  
Started Solr server on port 8983 (pid=6563). Happy searching!
```

提示：启动 Solr 前，需要提前启动 **Zookeeper** 服务。

### 查看Solr页面

9）Web 访问 8983 端口，可指定三台节点中的任意一台 IP，http://192.168.28.116:8983/

提示：UI 界面出现 **Cloud** 菜单栏时，Solr 的 Cloud 模式才算部署成功。

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618974787596-d7e653e2-de75-4ff8-b12d-683c5f992dea.png)

# 🎾Atlas

1、把 apache-atlas-2.0.0-server.tar.gz  上传到 tsingdata01的/opt/software 目录下

2、解压 apache-atlas-2.0.0-server.tar.gz  到/opt/module/目录下面

```
cd /opt/software
tar -zxvf apache-atlas-2.0.0-server.tar.gz -C /opt/module/
```

3、修改 apache-atlas-2.0.0 的名称为 atlas

```
cd /opt/module
mv apache-atlas-2.0.0/ atlas
```

## 💎Atlas集成外部框架

### Atlas 集成 Hbase

 

1）进入/opt/module/atlas/conf/目录，修改配置文件

```
vim atlas-application.properties
```



```
#修改atlas存储数据主机
atlas.graph.storage.hostname=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```

2）进入到/opt/module/atlas/conf/hbase 路径，添加 Hbase 集群的配置文件到${Atlas_Home} 

```
ln -s /opt/module/hbase-2.0.5/conf/ /opt/module/atlas/conf/hbase/conf
```

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618987130418-87e6cb57-375b-4330-a506-9731138f8736.png)

3）在/opt/module/atlas/conf/atlas-env.sh 中添加 HBASE_CONF_DIR

```
vim atlas-env.sh
#添加 HBase 配置文件路径
export HBASE_CONF_DIR=/opt/module/atlas/conf/hbase/conf
```

### Atlas 集成 Solr



1）进入/opt/module/atlas/conf 目录，修改配置文件

```
vim atlas-application.properties
#修改如下配置
atlas.graph.index.search.solr.zookeeper-url=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```

2）将 Atlas 自带的 Solr 文件夹拷贝到外部 Solr 集群的各个节点。

```
cp -r /opt/module/atlas/conf/solr /opt/module/solr/
```

3）进入到/opt/module/solr 路径，修改拷贝过来的配置文件名称为 atlas_conf

```
mv solr atlas_conf
```

4）在 Cloud 模式下，启动 Solr（需要提前启动 Zookeeper 集群），并创建 collection

```
cd /opt/module/solr
bin/solr create -c vertex_index -d /opt/module/solr/atlas_conf -shards 3 -replicationFactor 2 
bin/solr create -c edge_index -d /opt/module/solr/atlas_conf -shards 3 -replicationFactor 2 
bin/solr create -c fulltext_index -d /opt/module/solr/atlas_conf -shards 3 -replicationFactor 2
```

-shards 3：表示该集合分片数为 3

-replicationFactor 2：表示每个分片数都有 2 个备份

vertex_index、edge_index、fulltext_index：表示集合名称



注意：如果需要删除 vertex_index、edge_index、fulltext_index 等 collection 可以执行如下命令。

```
bin/solr delete -c vertex_index
bin/solr delete -c edge_index
bin/solr delete -c fulltext_index
```

5）验证创建 collection 成功

登录 solr web 控制台：[http://192.168.28.116:8983](http://192.168.157.128:8983/solr/#/~cloud) 看到如下图显示：

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618987962292-a3ded092-1398-44f9-a2d5-56741be40e5d.png)

### Atlas 集成 Kafka



1）进入/opt/module/atlas/conf/目录，修改配置文件 atlas-application.properties

```
vim atlas-application.properties
```



```
#########	Notification Configs	#########

atlas.notification.embedded=false

atlas.kafka.data=/opt/module/kafka/logs

atlas.kafka.zookeeper.connect=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181/kafka

atlas.kafka.bootstrap.servers=tsingdata01:9092,tsingdata02:9092,tsingdata03:9092
atlas.kafka.zookeeper.session.timeout.ms=4000 
atlas.kafka.zookeeper.connection.timeout.ms=2000

atlas.kafka.enable.auto.commit=true
```

2）启动 Kafka 集群，并创建 Topic

```
cd /opt/module/kafka
bin/kafka-topics.sh	--zookeeper tsingdata01:2181,tsingdata02:2181,tsingdata03:2181 --create --replication-factor 3 --partitions 3 --topic _HOATLASOK
bin/kafka-topics.sh	--zookeeper tsingdata01:2181,tsingdata02:2181,tsingdata03:2181 --create --replication-factor 3 --partitions 3 --topic ATLAS_ENTITIES
```



### Atlas 其他配置



1）进入/opt/module/atlas/conf/目录，修改配置文件 atlas-application.properties

```
vim atlas-application.properties
######### Server Properties ######### 
atlas.rest.address=http://tsingdata01:21000
#If enabled and set to true, this will run setup steps when the server starts
atlas.server.run.setup.on.start=false

#########	Entity Audit Configs	#########
atlas.audit.hbase.zookeeper.quorum=tsingdata01:2181,tsingdata02:2181,tsingdata03:2181
```



2）记录性能指标，进入/opt/module/atlas/conf/路径，修改当前目录下的 atlas-log4j.xml 

```
vim atlas-log4j.xml
#去掉如下代码的注释

<appender name="perf_appender" class="org.apache.log4j.DailyRollingFileAppender">
  <param name="file" value="${atlas.log.dir}/atlas_perf.log" /> <param name="datePattern" value="'.'yyyy-MM-dd" /> <param name="append" value="true" />
  <layout class="org.apache.log4j.PatternLayout">
  <param name="ConversionPattern" value="%d|%t|%m%n" /> </layout>
</appender>

<logger name="org.apache.atlas.perf" additivity="false"> <level value="debug" />
	<appender-ref ref="perf_appender" />
</logger>
```

### Atlas 集成 Hive

1）进入/opt/module/atlas/conf/目录，修改配置文件 atlas-application.properties

```
vim atlas-application.properties
######### Hive Hook Configs 直接添加#######
atlas.hook.hive.synchronous=false
atlas.hook.hive.numRetries=3
atlas.hook.hive.queueSize=10000
atlas.cluster.name=primary
```

2）解压 apache-atlas-2.0.0-hive-hook.tar.gz 到/opt/module/

```
tar -zxvf apache-atlas-2.0.0-hive-hook.tar.gz -C /opt/module/
```

3）剪切 hook 和 hook-bin 目录到到/opt/module/atlas 文件夹中

```
mv /opt/module/apache-atlas-hive-hook-2.0.0/hook-bin/ /opt/module/atlas/ 
mv /opt/module/apache-atlas-hive-hook-2.0.0/hook /opt/module/atlas/
```

4）将 atlas-application.properties 配置文件加入到 atlas-plugin-classloader-1.0.0.jar 中

```
zip -u /opt/module/atlas/hook/hive/atlas-plugin-classloader-2.0.0.jar /opt/module/atlas/conf/atlas-application.properties
cp /opt/module/atlas/conf/atlas-application.properties /opt/module/hive/conf/
```

原因：这个配置不能参照官网，将配置文件考到 hive 的 conf 中。参考官网的做法一直读取不到 atlas-application.properties 配置文件，看了源码发现是在 classpath 读取的这个配置文件，所以将它压到 jar 里面。

5）在/opt/module/hive/conf/hive-site.xml 文件中设置 Atlas hook

```
vim hive-site.xml
<property>
  <name>hive.exec.post.hooks</name>
  <value>org.apache.atlas.hive.hook.HiveHook</value>
</property>
vim hive-env.sh
#在 tez 引擎依赖的 jar 包后面追加 hive 插件相关 jar 包
export HIVE_AUX_JARS_PATH=/opt/module/atlas/hook/hive
```

## 🚢快速启动

```
hdp start
zk.sh start
kf.sh start
hbase.sh start
solr.sh start
```

进入/opt/module/atlas 路径，重新启动 **Atlas** 服务

```
cd /opt/module/atlas
bin/atlas_start.py
```

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618994732853-c0aa2e64-ab00-4d8f-b33c-d2a845952cca.png)

提示：错误信息查看路径：/opt/module/atlas/logs/*.out 和 application.log



我遇到的问题1：虽然提示服务启动，但是无法访问http://192.168.28.116:21000

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618996560524-2cf3a66f-a46a-42a9-ade4-599736a64177.png)

然后查看application.log日志：

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1618995952576-ea31aa87-ce70-4735-8035-235223b9bf16.png)

检查后发现是atlas.graph.storage.hostname=处多添加了一些，删掉即可。



遇到的问题2：

2021-04-22 12:28:32,338 ERROR - [main:] ~ GraphBackedSearchIndexer.initialize() failed (GraphBackedSearchIndexer:307)

org.apache.solr.common.SolrException: Could not load collection from ZK: vertex_index

很明显是solr的问题，删掉collection再创建即可。详见上面Atlas集成Solr

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1619061159521-ed0aeab0-ae0c-424c-9b72-607387c89b64.png)

访问地址：http://192.168.28.116:21000

注意：等待时间大概 3 分钟。启动非常慢！！！

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1619069873360-052cdaf5-1b8e-4605-a4ec-87fd34173ebd.png)

```
账户：admin
密码：admin
```

# Kylin

## 版本选型

```
3.0.2
```

## 安装

### 安装地址

官网地址：http://kylin.apache.org/cn/
官方文档：http://kylin.apache.org/cn/docs/
下载地址：http://kylin.apache.org/cn/download/

### 安装部署

1）下载并上传到/opt/software

2）解压 apache-kylin-3.0.2-bin.tar.gz 到 /opt/module

```
tar -zxvf apache-kylin-3.0.2-bin.tar.gz -C /opt/module/

cd /opt/module
mv apache-kylin-3.0.2-bin kylin
```



`注意`：需要在/etc/profile文件中配置HADOOP_HOME，HIVE_HOME，HBASE_HOME并将其对应的 sbin（如果有这个目录的话）和bin目录配置到Path，最后需要source使其生效。

```
vim /etc/profile
```



3）配置环境

```
#hadoop
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin

#hive
export HIVE_HOME=/opt/module/hive
export HIVE_CONF=/opt/module/hive/conf
export PATH=$PATH:$HIVE_HOME/bin

#hbase
export HBASE_HOME=/opt/module/hbase-2.0.5
export PATH=$PATH:$HBASE_HOME/bin

#kylin
export KYLIN_HOME=/opt/module/kylin
export PATH=$PATH:$KYLIN_HOME/bin

#spark目录
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```



```
source /etc/profile
```



4）兼容性问题

由于/opt/module/spark/jars中的hive依赖版本与我们使用的hive冲突

```
[root@tsingdata01 jars]# ls -al | grep hive
-rw-r--r--.  1 tsing-data tsing-data   138464 9月   8 2020 hive-beeline-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data    40817 9月   8 2020 hive-cli-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data 11498852 9月   8 2020 hive-exec-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data   100680 9月   8 2020 hive-jdbc-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data  5505200 9月   8 2020 hive-metastore-1.2.1.spark2.jar
-rw-r--r--.  1 tsing-data tsing-data  1565700 9月   8 2020 orc-core-1.5.5-nohive.jar
-rw-r--r--.  1 tsing-data tsing-data   812313 9月   8 2020 orc-mapreduce-1.5.5-nohive.jar
-rw-r--r--.  1 tsing-data tsing-data  1358996 9月   8 2020 spark-hive_2.11-2.4.7.jar
-rw-r--r--.  1 tsing-data tsing-data  1815976 9月   8 2020 spark-hive-thriftserver_2.11-2.4.7.jar
```

需要通过修改脚本排除依赖冲突

```
cd /opt/module/kylin/bin
vim find-hive-dependency.sh
hive_lib=`find -L ${hive_lib_dir} -name '*.jar' ! -name '*druid*' ! -name '*jackson*' ! -name '*metastore*' !-name '*slf4j*' ! -name '*avatica*' ! -name '*calcite*' ! -name '*jackson-datatype-joda*' ! -name '*derby*' -printf '%p:' | sed 's/:$//'`
vim find-spark-dependency.sh
spark_dependency=`find -L $spark_home/jars -name '*.jar' ! -name '*jackson*' ! -name '*metastore*' ! -name '*slf4j*' ! -name '*calcite*' ! -name '*doc*' ! -name '*test*' ! -name '*sources*' ''-printf '%p:' | sed 's/:$//'`
```

如果是第二次启动，先删除缓存

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1620874122779-90a8ebae-2e10-4e00-94d7-5e07b0f322d6.png)

5）启动

启动Kylin之前，需先启动Hadoop（hdfs, yarn, jobhistoryserver）、Zookeeper、Hbase

```
# 启动hadoop
hdp start

# 启动zk
zk.sh start

# 启动hbase
start-hbase.sh

# hive
nohup hive --service metastore &
hive

# 启动
kylin.sh start
```

说明启动成功了

![img](https://cdn.nlark.com/yuque/0/2021/png/519413/1620874167724-0f9a2e69-6f19-41d5-9297-92d377e476f8.png)

启动之后查看各个节点进程：

```
xcall jps
```



```
--------------------- tsingdata01 ----------------
3360 JobHistoryServer(MR的历史服务，必须启动)
31425 HMaster
3282 NodeManager
3026 DataNode
53283 Jps
2886 NameNode
44007 RunJar
2728 QuorumPeerMain
31566 HRegionServer
--------------------- tsingdata02 ----------------
5040 HMaster
2864 ResourceManager
9729 Jps
2657 QuorumPeerMain
4946 HRegionServer
2979 NodeManager
2727 DataNode
--------------------- tsingdata03 ----------------
4688 HRegionServer
2900 NodeManager
9848 Jps
2636 QuorumPeerMain
2700 DataNode
2815 SecondaryNameNode
```



访问：http://192.168.28.116:7070/kylin

```
用户名	ADMIN
密码	KYLIN
```



6）关闭

```
bin/kylin.sh stop
```





# Superset

## 前置要求



```
- Python3
- Anaconda/miniconda
```

### 安装Anaconda

https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/

下载上传到tsingdata02的/opt/software

```
bash Miniconda3-latest-Linux-x86_64.sh
```



一路回车、yes

![img](https://cdn.nlark.com/yuque/0/2020/png/519413/1599734808927-b3da7918-4a09-4679-9239-e59c04ff8064.png)

默认路径：/root/miniconda3

我们要修改为  /opt/module/miniconda3

![img](https://cdn.nlark.com/yuque/0/2020/png/519413/1599734891859-907e3bb9-2c43-41a4-9034-de1b6c717f90.png)

安装完毕！

添加为最新miniconda路径

```
vim ~/.bashrc
export PATH="/opt/module/miniconda3/bin:$PATH"
source ~/.bashrc
```

重新打开，base为默认环境  3.9.1 是默认的python版本

```
(base) [root@tsingdata01 software]# python
Python 3.9.1 (default, Dec 11 2020, 14:32:07) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
```

是否每次登陆自动激活base环境，

```
conda config --set auto_activate_base false
```

再次开启就不会有base了

激活base等环境

```
conda activate 需要激活的环境，如base
```

## 安装Superset

### 环境

使用pip或者conda（Python的包管理工具），和centos的yum类似。

配置conda国内镜像



```
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
```



显示下载地址：

```
conda config --set show_channel_urls yes
```

创建python3.6环境（不创建而使用默认的也可以）

```
conda create -name superset python=3.6  # 退出base环境，或者重启再执行这个命令
[root@tsingdata01 ~]# conda activate superset
(superset) [root@tsingdata01 ~]#
```

conda环境管理命令

```
1. 创建环境：conda create -n env_name

2. 查看所有环境：conda info -envs

3. 删除环境: conda remove -n env_name --all
```

激活虚拟环境

```
conda activate
```

关闭虚拟环境

```
conda deactivate
```

### 部署Superset

官网 [http://superset.apache.org/installation.html](http://superset.apache.org/installation.html#getting-started)

安装python 和 其他依赖

```
yum install -y python-setuptools
yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel
```



`遇到的问题`：-bash: /usr/bin/yum: /usr/bin/python: 坏的解释器: 没有那个文件或目录

`解决方法`：

修改以下配置文件

```
vim /usr/bin/yum
vim /usr/libexec/urlgrabber-ext-down
#!/usr/bin/python2 -> #!/usr/bin/python2.7
```

setuptools pip升级至最新版

```
pip install --upgrade setuptools pip -i https://pypi.douban.com/simple
```

安装Superset

```
pip install apache-superset -i https://pypi.douban.com/simple
```

初始化superset数据库

```
superset db upgrade
```

创建管理员用户

```
export FLASK_APP=superset
flask fab create-admin
```



```
[root@tsingdata02 ~]# flask fab create-admin
Username [admin]: tsingdata
User first name [admin]: 
User last name [user]: 
Email [admin@fab.org]: 
Password: 12345678
Repeat for confirmation: 
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
/opt/module/miniconda3/lib/python3.9/site-packages/flask_caching/__init__.py:201: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
  warnings.warn(
No PIL installation found
INFO:superset.utils.screenshots:No PIL installation found
Recognized Database Authentications.
Admin User tsingdata created.
```

初始化

```
superset init
```

安装gunicorn，这是类似tomcat的python web服务器

```
pip install gunicorn -i https://pypi.douban.com/simple
```



**启动**



```
gunicorn --workers 5 --timeout 120 --bind tsingdata01:8787 "superset.app:create_app()" --daemon
```



- workers：指定进程个数
- timeout： worker 进程超时时间，超时会自动重启

- bind：绑定本机地址，即为Superset访问地址
- daemon：后台运行4



访问：http://192.168.28.116:8787/

```
用户名 tsingdata
密码	12345678
```

停止

```
ps -ef | awk '/superset/ && !/awk/{print $2}' | xargs kill -9
```

